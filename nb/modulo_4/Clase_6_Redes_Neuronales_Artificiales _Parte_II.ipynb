{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Diplomatura en Ciencia de Datos - UNNE - 2024**\n",
    "### Módulo 4: Aprendizaje Automático\n",
    "### Clase 6: Redes Neuronales Artificiales con técnicas de regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dropout**\n",
    "\n",
    "Vamos a comparar el entrenamiento de una red con y sin dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_openml\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "\n",
    "np.random.seed(54)\n",
    "random.seed(54)\n",
    "torch.manual_seed(21)\n",
    "\n",
    "\n",
    "# Fetch the Heart Disease dataset\n",
    "data = fetch_openml(name=\"heart-disease\", version=1)\n",
    "X = data['data'].drop(['target'], axis=1)\n",
    "y = data['data']['target']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "# Neural Network (without Dropout)\n",
    "class HeartNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HeartNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_tensor.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Binary cross-entropy loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Mini-batch size, learning rate, and epochs\n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "n_epochs = 100\n",
    "\n",
    "# 10-Fold Cross Validation\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X_tensor)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    \n",
    "    # Initialize the network\n",
    "    modelo = HeartNet()\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X_tensor[train_idx], X_tensor[test_idx]\n",
    "    y_train, y_test = y_tensor[train_idx], y_tensor[test_idx]\n",
    "\n",
    "    # Create DataLoader for mini-batch SGD\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training process with mini-batch SGD\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Forward pass\n",
    "            y_pred = modelo(X_batch).squeeze()  # Predictions\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            # Backward pass and weight update\n",
    "            modelo.zero_grad()  # Clear previous gradients\n",
    "            loss.backward()  # Compute gradients\n",
    "            \n",
    "            # Update weights manually\n",
    "            with torch.no_grad():\n",
    "                for param in modelo.parameters():\n",
    "                    param -= learning_rate * param.grad\n",
    "        \n",
    "    # Evaluate on the test set and calculate accuracy\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = torch.sigmoid(modelo(X_test).squeeze())\n",
    "        y_test_pred = (y_test_pred >= 0.5).float()  # Convert to binary classification\n",
    "        accuracy = (y_test_pred == y_test).float().mean().item()  # Compute accuracy\n",
    "        fold_results.append(accuracy)\n",
    "        print(f'Accuracy for fold {fold + 1}: {accuracy:.4f}')\n",
    "\n",
    "# Final accuracy\n",
    "print(f'Mean Accuracy across all folds: {np.mean(fold_results):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network with Dropout\n",
    "class HeartNetDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HeartNetDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_tensor.shape[1], 64)\n",
    "        self.dropout1 = nn.Dropout(0.2)  # Dropout layer\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(0.2)  # Dropout layer\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Repeating the training and evaluation process with the dropout model\n",
    "fold_results_dropout = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X_tensor)):\n",
    "    print(f'Fold {fold + 1} (Dropout)')\n",
    "    \n",
    "    # Initialize the network with Dropout\n",
    "    model = HeartNetDropout()\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X_tensor[train_idx], X_tensor[test_idx]\n",
    "    y_train, y_test = y_tensor[train_idx], y_tensor[test_idx]\n",
    "\n",
    "    # Create DataLoader for mini-batch SGD\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training process with mini-batch SGD\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Forward pass\n",
    "            y_pred = model(X_batch).squeeze()  # Predictions\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            # Backward pass and weight update\n",
    "            model.zero_grad()  # Clear previous gradients\n",
    "            loss.backward()  # Compute gradients\n",
    "            \n",
    "            # Update weights manually\n",
    "            with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                    param -= learning_rate * param.grad\n",
    "    \n",
    "    # Evaluate on the test set and calculate accuracy\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = torch.sigmoid(model(X_test).squeeze())\n",
    "        y_test_pred = (y_test_pred >= 0.5).float()  # Convert to binary classification\n",
    "        accuracy = (y_test_pred == y_test).float().mean().item()  # Compute accuracy\n",
    "        fold_results_dropout.append(accuracy)\n",
    "        print(f'Accuracy for fold {fold + 1} (Dropout): {accuracy:.4f}')\n",
    "\n",
    "# Final accuracy with Dropout\n",
    "print(f'Mean Accuracy across all folds (Dropout): {np.mean(fold_results_dropout):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Inicialización Xavier**\n",
    "\n",
    "Vamos a comparar dos modelos, con y sin inicialialización Xavier / Glorot, para funciones de activación distintas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch.manual_seed(771142090864900)\n",
    "\n",
    "# Create a synthetic dataset for binary classification\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation='sigmoid', xavier_init=False):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Choose activation function\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        # Apply Xavier initialization if specified\n",
    "        if xavier_init:\n",
    "            torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "            torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, X_train, y_train, X_test, y_test, num_epochs=100, learning_rate=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training step\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Testing step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 2  # Number of features in the dataset\n",
    "hidden_dim = 10  # Number of hidden units\n",
    "output_dim = 2  # Binary classification (2 classes)\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize models with and without Xavier initialization\n",
    "model_no_xavier_sigmoid = SimpleNN(input_dim, hidden_dim, output_dim, activation='sigmoid', xavier_init=False)\n",
    "model_xavier_sigmoid = SimpleNN(input_dim, hidden_dim, output_dim, activation='sigmoid', xavier_init=True)\n",
    "\n",
    "model_no_xavier_tanh = SimpleNN(input_dim, hidden_dim, output_dim, activation='tanh', xavier_init=False)\n",
    "model_xavier_tanh = SimpleNN(input_dim, hidden_dim, output_dim, activation='tanh', xavier_init=True)\n",
    "\n",
    "# Train the models\n",
    "print(\"\\nTraining model without Xavier initialization (Sigmoid)...\")\n",
    "train_losses_no_xavier_sigmoid, test_losses_no_xavier_sigmoid = train_model(\n",
    "    model_no_xavier_sigmoid, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, num_epochs, learning_rate)\n",
    "\n",
    "print(\"\\nTraining model with Xavier initialization (Sigmoid)...\")\n",
    "train_losses_xavier_sigmoid, test_losses_xavier_sigmoid = train_model(\n",
    "    model_xavier_sigmoid, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, num_epochs, learning_rate)\n",
    "\n",
    "print(\"\\nTraining model without Xavier initialization (Tanh)...\")\n",
    "train_losses_no_xavier_tanh, test_losses_no_xavier_tanh = train_model(\n",
    "    model_no_xavier_tanh, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, num_epochs, learning_rate)\n",
    "\n",
    "print(\"\\nTraining model with Xavier initialization (Tanh)...\")\n",
    "train_losses_xavier_tanh, test_losses_xavier_tanh = train_model(\n",
    "    model_xavier_tanh, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, num_epochs, learning_rate)\n",
    "\n",
    "# Plot the training and test loss comparison\n",
    "epochs = range(1, num_epochs+1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Sigmoid Activation\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, test_losses_no_xavier_sigmoid, label='No Xavier', color='blue')\n",
    "plt.plot(epochs, test_losses_xavier_sigmoid, label='Xavier', color='orange')\n",
    "plt.title('Sigmoid Activation - Xavier vs No Xavier')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Tanh Activation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, test_losses_no_xavier_tanh, label='No Xavier', color='blue')\n",
    "plt.plot(epochs, test_losses_xavier_tanh, label='Xavier', color='orange')\n",
    "plt.title('Tanh Activation - Xavier vs No Xavier')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Inicialización He**\n",
    "\n",
    "Vamos a ver el efecto de la inicialización He / Klaiming sobre la función ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a synthetic dataset for binary classification\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Define a simple feedforward neural network with He initialization and ReLU activation\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, he_init=False):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Apply He initialization if specified\n",
    "        if he_init:\n",
    "            torch.nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "            torch.nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train the model without using optimizers (manual gradient descent)\n",
    "def train_model(model, X_train, y_train, X_test, y_test, num_epochs=100, learning_rate=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        # Backward pass and weight update (manual gradient descent)\n",
    "        model.zero_grad()  # Zero out previous gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Manually update weights using gradient descent\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Testing step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 2  # Number of features in the dataset\n",
    "hidden_dim = 10  # Number of hidden units\n",
    "output_dim = 2  # Binary classification (2 classes)\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize models with and without He initialization\n",
    "model_no_he_relu = SimpleNN(input_dim, hidden_dim, output_dim, he_init=False)\n",
    "model_he_relu = SimpleNN(input_dim, hidden_dim, output_dim, he_init=True)\n",
    "\n",
    "# Train the models\n",
    "print(\"\\nTraining model without He initialization (ReLU)...\")\n",
    "train_losses_no_he_relu, test_losses_no_he_relu = train_model(\n",
    "    model_no_he_relu, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, num_epochs, learning_rate)\n",
    "\n",
    "print(\"\\nTraining model with He initialization (ReLU)...\")\n",
    "train_losses_he_relu, test_losses_he_relu = train_model(\n",
    "    model_he_relu, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, num_epochs, learning_rate)\n",
    "\n",
    "# Plot the training and test loss comparison\n",
    "epochs = range(1, num_epochs+1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# ReLU Activation with and without He Initialization\n",
    "plt.plot(epochs, test_losses_no_he_relu, label='No He', color='blue')\n",
    "plt.plot(epochs, test_losses_he_relu, label='He', color='orange')\n",
    "\n",
    "plt.title('ReLU Activation - He vs No He Initialization')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset de cancer de mama**\n",
    "\n",
    "Vamos a usar los conceptos introducidos en esta clase en un dataset real. Es un caso de clasificación binaria en casos benignos y malignos.\n",
    "\n",
    "ID: Número de identificación del paciente.\n",
    "\n",
    "- Diagnóstico (Diagnosis): Variable objetivo que indica si la muestra es benigna (B) o maligna (M).\n",
    "\n",
    "10 características de valor real (real-valued features) que describen diferentes propiedades de los núcleos celulares en las imágenes FNA. Estas características incluyen:\n",
    "- Radio (radius)\n",
    "- Textura (texture)\n",
    "- Perímetro (perimeter)\n",
    "- Área (area)\n",
    "- Suavidad (smoothness)\n",
    "- Compacidad (compactness)\n",
    "- Concavidad (concavity)\n",
    "- Puntos cóncavos (concave points)\n",
    "- Simetría (symmetry)\n",
    "- Dimensión fractal (fractal dimension)\n",
    "\n",
    "\n",
    "The mean, standard error, and “worst” or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load a real-life dataset (Breast Cancer dataset)\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create a DataLoader for mini-batch training\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define a neural network with two hidden layers\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "        # Kaiming Initialization\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "output_size = 2  # Binary classification\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "l1_lambda = 0.001\n",
    "clip_value = 1.0\n",
    "\n",
    "# Initialize the model, loss function, and manually handle SGD\n",
    "model = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# For plotting\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    accuracy = correct / labels.size(0)\n",
    "    return accuracy\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # L1 regularization\n",
    "        l1_norm = sum(torch.sum(torch.abs(param)) for param in model.parameters())\n",
    "        loss += l1_lambda * l1_norm\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        \n",
    "        # Manually update weights\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "        \n",
    "        # Zero the gradients after updating\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Update accuracy for training data\n",
    "        total_loss += loss.item()\n",
    "        correct_train += (torch.argmax(outputs, dim=1) == y_batch).sum().item()\n",
    "        total_train += y_batch.size(0)\n",
    "    \n",
    "    # Calculate and record training accuracy\n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor)\n",
    "        test_loss = criterion(test_outputs, y_test_tensor).item()\n",
    "        \n",
    "        # Calculate test accuracy\n",
    "        test_accuracy = calculate_accuracy(test_outputs, y_test_tensor)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    # Print accuracy every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs}], Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Plot the training and testing accuracy curves\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(test_accuracies, label='Testing Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Testing Accuracy Curves')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Wine Dataset: clasificación multi-clase**\n",
    "\n",
    "**Data Set Characteristics**:\n",
    "\n",
    "**Number of Instances**: 178\n",
    "\n",
    "**Number of Attributes**: 13 numeric, predictive attributes and the class\n",
    "\n",
    "**Attribute Information**:\n",
    "1. Alcohol\n",
    "\n",
    "2. Malic acid\n",
    "\n",
    "3. Ash\n",
    "\n",
    "4. Alcalinity of ash\n",
    "\n",
    "5. Magnesium\n",
    "\n",
    "6. Total phenols\n",
    "\n",
    "7. Flavanoids\n",
    "\n",
    "8. Nonflavanoid phenols\n",
    "\n",
    "9. Proanthocyanins\n",
    "\n",
    "10. Color intensity\n",
    "\n",
    "11. Hue\n",
    "\n",
    "12. OD280/OD315 of diluted wines\n",
    "\n",
    "13. Proline\n",
    "\n",
    "**class**:\n",
    "1. class_0\n",
    "\n",
    "2. class_1\n",
    "\n",
    "3. class_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create a DataLoader for mini-batch training\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define a neural network with two hidden layers\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "        # Kaiming Initialization\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "output_size = len(data.target_names)  # 3 classes for Wine dataset\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "clip_value = 1.0\n",
    "\n",
    "# Initialize the model, loss function, and manually handle SGD\n",
    "model = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# For plotting\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        \n",
    "        # Manually update weights\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "        \n",
    "        # Zero the gradients after updating\n",
    "        model.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Record training loss\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor)\n",
    "        test_loss = criterion(test_outputs, y_test_tensor).item()\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs}], Training Loss: {avg_train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Plot the training and testing loss curves\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Testing Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Diabetes Dataset : regresión**\n",
    "\n",
    "Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n",
    "\n",
    "**Dataset characteristics:**\n",
    "\n",
    "**Number of Instances**: 442\n",
    "\n",
    "**Number of Attributes**: First 10 columns are numeric predictive values\n",
    "\n",
    "**Target**: Column 11 is a quantitative measure of disease progression one year after baseline\n",
    "\n",
    "**Attribute Information**:\n",
    "\n",
    "1. Age: age in years\n",
    "\n",
    "2. Sex\n",
    "\n",
    "3. BMI: body mass index\n",
    "\n",
    "4. BP: average blood pressure\n",
    "\n",
    "5. S1: tc, total serum cholesterol\n",
    "\n",
    "6. S2: ldl, low-density lipoproteins\n",
    "\n",
    "7. S3: hdl, high-density lipoproteins\n",
    "\n",
    "8. S4: tch, total cholesterol / HDL\n",
    "\n",
    "9. S5: ltg, possibly log of serum triglycerides level\n",
    "\n",
    "10. S6: glu, blood sugar level\n",
    "\n",
    "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of n_samples (i.e. the sum of squares of each column totals 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load a regression dataset (Diabetes)\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create a DataLoader for mini-batch training\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define a neural network with two hidden layers\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "        # Kaiming Initialization\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "output_size = 1  # Regression output\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "clip_value = 0.5\n",
    "\n",
    "# Initialize the model, loss function, and manually handle SGD\n",
    "model = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# For plotting\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        \n",
    "        # Manually update weights\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "        \n",
    "        # Zero the gradients after updating\n",
    "        model.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Record training loss\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor)\n",
    "        test_loss = criterion(test_outputs, y_test_tensor).item()\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs}], Training Loss: {avg_train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Plot the training and testing loss curves\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Testing Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Testing Loss Curves')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
