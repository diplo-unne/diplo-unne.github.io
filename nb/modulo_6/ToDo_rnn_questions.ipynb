{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Módulo 6 - Aprendizaje profundo\n",
        "## Clase 3: RNN"
      ],
      "metadata": {
        "id": "9DUH_LpeCZm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo 1"
      ],
      "metadata": {
        "id": "7CFpskLkmwXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplos 1)\n",
        "Modelo de clasificación de preguntas y respuestas\n",
        "\n",
        "En esta práctica trabajaremos con un modelo de clasificación de texto, en el cual se procesarán preguntas sobre temas médicos para predecir la respuesta más probable entre una serie de respuestas posibles.\n",
        "\n",
        "Este dataset contiene preguntas y respuestas relacionadas con temas de salud pública y enfermedades infecciosas, incluyendo VIH, neumonía, MERS-CoV, H1N1, hepatitis y otros.\n",
        "\n",
        "## Pasos de la práctica\n",
        "\n",
        "1. **Carga y limpieza de datos**: Usaremos un dataset con preguntas y respuestas reales. Primero, descargaremos estos datos y los limpiaremos, eliminando caracteres especiales y pasando todo el texto a minúsculas para facilitar el procesamiento.\n",
        "\n",
        "2. **Preprocesamiento**: Convertiremos el texto en secuencias de números. Para ello, crearemos un \"vocabulario\", es decir, un diccionario donde cada palabra del dataset tendrá un número asociado. Esta etapa es clave para que el modelo entienda el texto.\n",
        "\n",
        "3. **Definición del modelo**: Crearemos un modelo simple de clasificación con una capa de **embeddings** que convierte palabras en vectores de características y una capa completamente conectada que predice la respuesta entre varias opciones.\n",
        "\n",
        "4. **Entrenamiento del modelo**: Usaremos un optimizador y una función de pérdida para entrenar el modelo. Durante el entrenamiento, el modelo intentará minimizar el error y mejorar la precisión de sus predicciones.\n",
        "\n",
        "5. **Evaluación del modelo**: Evaluaremos el modelo para verificar su rendimiento en el conjunto de prueba. Mediremos la precisión para observar qué tan bien puede el modelo responder preguntas nuevas.\n",
        "\n",
        "6. **Predicción de nuevas respuestas**: Finalmente, podrás probar el modelo con tus propias preguntas para ver la respuesta que genera en base al entrenamiento previo.\n",
        "\n",
        "## Hiperparámetros\n",
        "\n",
        "Durante esta práctica, tendrás la oportunidad de experimentar con los siguientes parámetros para observar su impacto en el modelo:\n",
        "\n",
        "- **max_len**: Longitud máxima de las secuencias de palabras. Define cuántas palabras considera el modelo en cada pregunta.\n",
        "- **embed_size**: Tamaño del vector de embedding, que representa cada palabra en un espacio numérico.\n",
        "- **batch_size**: Número de ejemplos que procesa el modelo en cada iteración.\n",
        "- **epochs**: Número de veces que el modelo recorre el dataset durante el entrenamiento.\n",
        "- **learning_rate**: Tasa de aprendizaje que determina qué tan grandes son los ajustes que el modelo realiza en cada paso de entrenamiento.\n",
        "\n",
        "## Procesos\n",
        "- **Vocabulario**: Transformamos texto en números únicos para que el modelo comprenda las palabras.\n",
        "- **Conversión**: Cambiamos cada palabra en una secuencia de índices, que el modelo puede procesar.\n",
        "- **Truncamiento**: Controlamos la longitud de las preguntas para que todas sean iguales.\n",
        "- **Padding**: Rellenamos con ceros las preguntas cortas para asegurar un tamaño constante de entrada."
      ],
      "metadata": {
        "id": "BXduHaZSER8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "#https://github.com/deepset-ai/COVID-QA/tree/master/data/question-answering\n",
        "\n",
        "print(\"Descargando dataset...\")\n",
        "url = 'https://drive.google.com/uc?export=download&id=1VBrfB70BATFUxtT1LZ-Z0Aq4Q_Ff9ZP0'\n",
        "destination = \"predictions.tsv\"\n",
        "gdown.download(url, destination, quiet=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "DJt_hpr1MnHD",
        "outputId": "86fd9929-7f7c-4d9c-ef90-469a50092d2a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1VBrfB70BATFUxtT1LZ-Z0Aq4Q_Ff9ZP0\n",
            "To: /content/predictions.tsv\n",
            "100%|██████████| 77.7k/77.7k [00:00<00:00, 24.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'predictions.tsv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cargar el dataset con pandas\n",
        "data = pd.read_csv(destination, sep='\\t')\n",
        "print(data.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggxKrh64AYs4",
        "outputId": "c872afa9-2db4-4edf-82d5-4b839b50002c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 249 entries, 0 to 248\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   question     249 non-null    object\n",
            " 1   real_answer  249 non-null    object\n",
            "dtypes: object(2)\n",
            "memory usage: 4.0+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo con un modelo simple\n",
        "\n",
        "\n",
        "Le mostramos un modelo inicial para que comprendan el flujo y la lógica usando el dataset proporcionado, antes de pasar a una versión más avanzada, que deben realizar.\n",
        "\n",
        "1. **Embeddings**: Representan cada palabra de la pregunta en un espacio vectorial, lo que facilita que el modelo aprenda relaciones entre palabras.\n",
        "2. **LSTM**: Procesa la secuencia de palabras y retiene información importante a lo largo de la frase.\n",
        "3. **Capa Fully Connected**: Finalmente, convierte la salida de la LSTM en una predicción de la respuesta.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_2yh7okzsVj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías necesarias\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import re\n",
        "\n",
        "# Hiperparámetros\n",
        "embed_size = 32          # Tamaño del embedding\n",
        "hidden_size = 32         # Tamaño de la capa oculta LSTM\n",
        "batch_size = 8           # Tamaño de cada lote de entrenamiento\n",
        "max_len = 10             # Longitud máxima de la secuencia de palabras\n",
        "learning_rate = 0.001    # Tasa de aprendizaje\n",
        "epochs = 10               # Número de épocas de entrenamiento\n",
        "\n",
        "# Cargar los datos y preprocesar texto\n",
        "data = pd.read_csv(\"predictions.tsv\", sep='\\t')\n",
        "data['question_clean'] = data['question'].apply(lambda x: re.sub(r\"[^a-z0-9\\s]\", '', x.lower()))\n",
        "data['answer_clean'] = data['real_answer'].apply(lambda x: re.sub(r\"[^a-z0-9\\s]\", '', x.lower()))\n",
        "\n",
        "# Codificar respuestas y texto\n",
        "le = LabelEncoder()\n",
        "data['label'] = le.fit_transform(data['answer_clean'])\n",
        "vocab = {w: i+1 for i, w in enumerate(set(' '.join(data['question_clean']).split()))}\n",
        "vocab['<pad>'] = 0\n",
        "#[\"hola cómo estás\"] -> [1, 2, 3] max_len = 3\n",
        "#[\"bien\"] -> [4] -> [1, 0, 0] \"bien\" + padding\n",
        "\n",
        "def convertir_a_int(text):\n",
        "    return [vocab.get(word, 0) for word in text.split()]\n",
        "\n",
        "# Convertir y rellenar preguntas con padding\n",
        "X = [convertir_a_int(text)[:max_len] + [0] * (max_len - len(text.split()[:max_len])) for text in data['question_clean']]\n",
        "y = data['label'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#print(X)\n",
        "#print(y)\n",
        "#print(vocab)\n",
        "\n",
        "# Convertir a tensores\n",
        "X_train, X_test = torch.tensor(X_train), torch.tensor(X_test)\n",
        "y_train, y_test = torch.tensor(y_train), torch.tensor(y_test)\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "# Definir un modelo de clasificación simple con LSTM\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "# Instanciar el modelo y la función de pérdida\n",
        "model = SimpleLSTM(len(vocab), embed_size, hidden_size, len(le.classes_))\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Entrenar el modelo\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Época {epoch+1}, Pérdida: {loss.item():.4f}\")\n",
        "\n",
        "# Función de predicción\n",
        "def predecir_respuesta(model, pregunta):\n",
        "    model.eval()\n",
        "    pregunta_int = convertir_a_int(re.sub(r\"[^a-z0-9\\s]\", '', pregunta.lower()))\n",
        "    pregunta_pad = torch.tensor([pregunta_int[:max_len] + [0] * (max_len - len(pregunta_int[:max_len]))])\n",
        "    with torch.no_grad():\n",
        "        output = model(pregunta_pad)\n",
        "        pred = output.argmax(dim=1).item()\n",
        "    return le.inverse_transform([pred])[0]\n",
        "\n",
        "# Probar el modelo con una pregunta nueva\n",
        "pregunta_nueva = \"¿Cuál es la causa principal de la transmisión del VIH?\"\n",
        "print(\"Respuesta predicha:\", predecir_respuesta(model, pregunta_nueva))\n",
        "\n",
        "#Pregunta original: \"¿Cuál es la causa principal de transmisión?\"\n",
        "#Secuencia numérica: [12, 23, 45, 34, 89]  # Índices de palabras\n",
        "#Secuencia con padding: [12, 23, 45, 34, 89, 0, 0, 0, 0, 0]  # Si max_len = 10\n",
        "#Respuestas únicas:\n",
        "#[\"La transmisión es aérea\", \"Por contacto directo\", \"Uso de objetos contaminados\"]\n",
        "#[0, 1, 3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYwA3unFpLKj",
        "outputId": "8a1ed2a8-cf0f-4c95-a2ca-35de495c098b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1, Pérdida: 4.3223\n",
            "Época 2, Pérdida: 4.2087\n",
            "Época 3, Pérdida: 4.3021\n",
            "Época 4, Pérdida: 4.1995\n",
            "Época 5, Pérdida: 4.1395\n",
            "Época 6, Pérdida: 3.8889\n",
            "Época 7, Pérdida: 3.7318\n",
            "Época 8, Pérdida: 3.0722\n",
            "Época 9, Pérdida: 2.8887\n",
            "Época 10, Pérdida: 2.5504\n",
            "Respuesta predicha: se ha introducido una definicin de caso revisada de neumona bacteriana presumida y esta definicin incluye casos de neumona con consolidacin alveolar definida por la oms as como aquellos con otras infiltraciones radiogrficas torcicas anormales y una protena c reactiva srica de al menos 40 mgl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar el modelo con una pregunta nueva\n",
        "#pregunta_nueva = \"¿ Qué virus de la influenza se identificó en China en 2013?\" # bien\n",
        "pregunta_nueva = \"virus de la influenza se identificó en China en 2013?\" # mal\n",
        "#pregunta_nueva = \"¿ Qué  virus de la influenza se descubrio en China en 2013?\" # bien\n",
        "#pregunta_nueva = \"¿ Qué  virus de la influenza se descubrio en 2013?\" # bien\n",
        "#pregunta_nueva = \"tratamiento para MERS-COV?\"\n",
        "respuesta_predicha = predecir_respuesta(model, pregunta_nueva)\n",
        "print(\"\\nPregunta:\", pregunta_nueva)\n",
        "print(\"Respuesta predicha:\", respuesta_predicha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnYeZLDz0YgR",
        "outputId": "0c757e5f-36a0-4bb3-f151-d7604fb2fcf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: virus de la influenza se identificó en China en 2013?\n",
            "Respuesta predicha: h7n9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad"
      ],
      "metadata": {
        "id": "HkvnVUYxmzYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO contruir un modelo LSTM\n",
        "\n",
        "Deben construir su propio modelo basado en una LSTM (Long Short-Term Memory) para clasificación de texto. Este modelo será muy similar a los ejemplos que han visto en el cuaderno `RNN(refranes_and_poemas).ipynb`, donde trabajaron con RNNs para generación de texto, y en la celda anterior, donde vimos un ejemplo simple de modelo para clasificación con  el dataset actual.\n",
        "\n",
        "**Objetivo**: Diseñar un modelo que pueda clasificar las preguntas en base a sus respuestas, utilizando una arquitectura LSTM. Este modelo tomará una secuencia de palabras y aprenderá a predecir la categoría o clase correspondiente a la respuesta.\n",
        "\n",
        "### Recomendaciones y Pautas\n",
        "\n",
        "Basado en el codigo anterior agregar lo siguiente:\n",
        "\n",
        "1. **Agregar capa de embeddings.**  \n",
        "2. **Implementar función de `accuracy`.**  \n",
        "3. **Mejorar limpieza con regex.**  \n",
        "4. **Incluir función `evaluate`.**  \n",
        "5. **Añadir Dropout a la LSTM.**  \n",
        "6. **Manejar estado oculto con `init_hidden`.**  \n",
        "7. **Usar `LabelEncoder` para etiquetas.**  \n",
        "8. **Incluir activación personalizada.**  \n",
        "9. **Permitir múltiples capas LSTM (`num_layers`).**  \n",
        "10. **Mostrar precisión y pérdida por época.**\n",
        "\n",
        "Suerte !!!\n"
      ],
      "metadata": {
        "id": "BfiWwwFethck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías necesarias\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder  # (7)\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import re\n",
        "\n",
        "# Hiperparámetros\n",
        "#...          # Tamaño del embedding (1)\n",
        "hidden_size = #...           # Tamaño de la capa oculta LSTM\n",
        "#...            # Número de capas LSTM (9)\n",
        "batch_size = #...          # Tamaño de cada lote de entrenamiento\n",
        "max_len = #...              # Longitud máxima de la secuencia de palabras\n",
        "learning_rate = #...      # Tasa de aprendizaje\n",
        "epochs = #...                # Número de épocas de entrenamiento\n",
        "\n",
        "# Función para limpiar texto (3)\n",
        "def limpiar_texto(texto):\n",
        "    \"\"\"Limpia el texto eliminando caracteres no deseados y convierte a minúsculas.\"\"\"\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r\"[^a-záéíóúüñ0-9\\s]\", '', texto)\n",
        "    return texto.strip()\n",
        "\n",
        "# Cargar los datos y preprocesar texto\n",
        "data = pd.read_csv(\"predictions.tsv\", sep='\\t')\n",
        "data['question_clean'] = data['question'].apply(limpiar_texto)  # Usamos la función limpiar_texto (3)\n",
        "data['answer_clean'] = #...\n",
        "\n",
        "# Codificar respuestas y texto\n",
        "le = LabelEncoder()  # Usar LabelEncoder para etiquetas (7)\n",
        "data['label'] = le.fit_transform(data['answer_clean'])\n",
        "\n",
        "# Crear vocabulario con padding\n",
        "vocab = {w: i+1 for i, w in enumerate(set(' '.join(data['question_clean']).split()))}\n",
        "vocab['<pad>'] = 0\n",
        "\n",
        "def convertir_a_int(text):\n",
        "    return [vocab.get(word, 0) for word in text.split()]\n",
        "\n",
        "# Convertir y rellenar preguntas con padding\n",
        "X = [\n",
        "    convertir_a_int(text)[:max_len] + [0] * (max_len - len(convertir_a_int(text)[:max_len]))\n",
        "    for text in data['question_clean']\n",
        "]\n",
        "y = data['label'].values\n",
        "\n",
        "# Dividir en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = #...\n",
        "\n",
        "# Convertir a tensores\n",
        "X_train = torch.tensor(X_train)\n",
        "X_test = #...\n",
        "y_train = torch.tensor(y_train)\n",
        "y_test = #...\n",
        "\n",
        "# Crear DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True\n",
        ")\n",
        "test_loader = #...\n",
        "\n",
        "# Definir un modelo de clasificación con LSTM y Dropout\n",
        "class CustomizableLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes, num_layers=1, dropout=0.0, activation='tanh'):\n",
        "        super(CustomizableLSTM, self).__init__()\n",
        "        #...   = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = #...   # Capa de embeddings (1)\n",
        "\n",
        "        # LSTM con múltiples capas y Dropout (5, 9)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_size, hidden_size, num_layers=#...\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear#...\n",
        "\n",
        "        # Activación personalizada (8)\n",
        "        if activation == 'tanh':\n",
        "            #...\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        x = #...\n",
        "        out, h = self.lstm(x, h)\n",
        "        out = self.activation(out[:, -1, :])  # Aplicamos la activación personalizada (8)\n",
        "        out = self.fc(out)\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Inicializar el estado oculto y de memoria (6)\n",
        "        return (\n",
        "            torch.zeros(self.num_layers, #...  ),\n",
        "            #...\n",
        "        )\n",
        "\n",
        "# Instanciar el modelo y la función de pérdida\n",
        "vocab_size = len(vocab)\n",
        "num_classes = len(le.classes_)\n",
        "model = CustomizableLSTM(vocab_size, embed_size, hidden_size, num_classes, num_layers=#... )\n",
        "optimizer = #...\n",
        "criterion = #...\n",
        "\n",
        "# Función para calcular precisión (2)\n",
        "def accuracy(outputs, labels):\n",
        "    preds = outputs.argmax(dim=1)\n",
        "    return (preds == labels).sum().item() / #...\n",
        "\n",
        "# Función para evaluar el modelo (4)\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_accuracy = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            h = model.init_hidden(inputs.size(0))  # Manejar estado oculto (6)\n",
        "            outputs, h = model(inputs, h)\n",
        "            loss = #...\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += #...\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    avg_accuracy = total_accuracy / len(loader)\n",
        "    return avg_loss, #...\n",
        "\n",
        "# Entrenar el modelo\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0  # Precisión en cada época (2)\n",
        "    for inputs, labels in train_loader:\n",
        "        h = model.init_hidden(inputs.size(0))  # Manejar estado oculto (6)\n",
        "        #...\n",
        "        outputs, h = model(inputs, h)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.#...\n",
        "        #...\n",
        "        total_loss += #...\n",
        "        total_accuracy += #...\n",
        "    avg_loss = total_loss / #...\n",
        "    avg_accuracy = total_accuracy / #...\n",
        "    print(f\"Época {epoch+1}, Pérdida de entrenamiento: {avg_loss:.4f}, Precisión de entrenamiento: {avg_accuracy:.4f}\")  # Mostrar precisión y pérdida por época (10)\n",
        "\n",
        "    # Evaluar después de cada época (4, 10)\n",
        "    val_loss, val_accuracy =#...\n",
        "    print(f\"Precisión en validación: {val_accuracy:.4f}, Pérdida en validación: {val_loss:.4f}\")\n",
        "\n",
        "# Función de predicción\n",
        "def predecir_respuesta(model, pregunta):\n",
        "    model.#...\n",
        "    pregunta_clean = limpiar_texto(pregunta)\n",
        "    pregunta_int = convertir_a_int(pregunta_clean)\n",
        "    pregunta_pad = [\n",
        "        pregunta_int[:max_len] + [0] * (max_len - len(pregunta_int[:max_len]))\n",
        "    ]\n",
        "    pregunta_tensor = torch.tensor(pregunta_pad)\n",
        "    h = model.init_hidden(1)  # Manejar estado oculto (6)\n",
        "    with #... :\n",
        "        output, h = #...\n",
        "        pred = output.argmax(dim=1).item()\n",
        "    return le.inverse_transform([pred])[0]\n",
        "\n",
        "# Probar el modelo con una pregunta nueva\n",
        "pregunta_nueva = \"¿Cuál es la causa principal de la transmisión del VIH?\"\n",
        "print(\"Respuesta predicha:\", predecir_respuesta(model, pregunta_nueva))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-QUtHt2NekJ",
        "outputId": "27d87245-78d0-4da6-ea96-8c7788673dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1, Pérdida de entrenamiento: 4.4192, Precisión de entrenamiento: 0.0200\n",
            "Precisión en validación: 0.0000, Pérdida en validación: 4.4428\n",
            "Época 2, Pérdida de entrenamiento: 4.3782, Precisión de entrenamiento: 0.0450\n",
            "Precisión en validación: 0.0000, Pérdida en validación: 4.4355\n",
            "Época 3, Pérdida de entrenamiento: 4.2869, Precisión de entrenamiento: 0.1414\n",
            "Precisión en validación: 0.0179, Pérdida en validación: 4.4054\n",
            "Época 4, Pérdida de entrenamiento: 3.9588, Precisión de entrenamiento: 0.2079\n",
            "Precisión en validación: 0.0179, Pérdida en validación: 4.2390\n",
            "Época 5, Pérdida de entrenamiento: 3.3693, Precisión de entrenamiento: 0.4064\n",
            "Precisión en validación: 0.1964, Pérdida en validación: 3.8358\n",
            "Época 6, Pérdida de entrenamiento: 2.8666, Precisión de entrenamiento: 0.5786\n",
            "Precisión en validación: 0.2679, Pérdida en validación: 3.4721\n",
            "Época 7, Pérdida de entrenamiento: 2.4223, Precisión de entrenamiento: 0.7386\n",
            "Precisión en validación: 0.3393, Pérdida en validación: 3.1779\n",
            "Época 8, Pérdida de entrenamiento: 2.0739, Precisión de entrenamiento: 0.7993\n",
            "Precisión en validación: 0.3929, Pérdida en validación: 2.9494\n",
            "Época 9, Pérdida de entrenamiento: 1.8039, Precisión de entrenamiento: 0.8236\n",
            "Precisión en validación: 0.3750, Pérdida en validación: 2.7703\n",
            "Época 10, Pérdida de entrenamiento: 1.5888, Precisión de entrenamiento: 0.8643\n",
            "Precisión en validación: 0.4286, Pérdida en validación: 2.4889\n",
            "Época 11, Pérdida de entrenamiento: 1.4018, Precisión de entrenamiento: 0.9186\n",
            "Precisión en validación: 0.5357, Pérdida en validación: 2.3749\n",
            "Época 12, Pérdida de entrenamiento: 1.2070, Precisión de entrenamiento: 0.9436\n",
            "Precisión en validación: 0.5714, Pérdida en validación: 2.1901\n",
            "Época 13, Pérdida de entrenamiento: 1.0657, Precisión de entrenamiento: 0.9450\n",
            "Precisión en validación: 0.6786, Pérdida en validación: 1.9966\n",
            "Época 14, Pérdida de entrenamiento: 0.9352, Precisión de entrenamiento: 0.9650\n",
            "Precisión en validación: 0.6607, Pérdida en validación: 1.9184\n",
            "Época 15, Pérdida de entrenamiento: 0.8264, Precisión de entrenamiento: 0.9650\n",
            "Precisión en validación: 0.7143, Pérdida en validación: 1.6972\n",
            "Época 16, Pérdida de entrenamiento: 0.7365, Precisión de entrenamiento: 0.9750\n",
            "Precisión en validación: 0.8036, Pérdida en validación: 1.6114\n",
            "Época 17, Pérdida de entrenamiento: 0.6392, Precisión de entrenamiento: 0.9850\n",
            "Precisión en validación: 0.8929, Pérdida en validación: 1.4849\n",
            "Época 18, Pérdida de entrenamiento: 0.5785, Precisión de entrenamiento: 0.9950\n",
            "Precisión en validación: 0.8929, Pérdida en validación: 1.3900\n",
            "Época 19, Pérdida de entrenamiento: 0.5260, Precisión de entrenamiento: 0.9900\n",
            "Precisión en validación: 0.8750, Pérdida en validación: 1.3156\n",
            "Época 20, Pérdida de entrenamiento: 0.4738, Precisión de entrenamiento: 0.9950\n",
            "Precisión en validación: 0.8750, Pérdida en validación: 1.2343\n",
            "Época 21, Pérdida de entrenamiento: 0.4227, Precisión de entrenamiento: 1.0000\n",
            "Precisión en validación: 0.9107, Pérdida en validación: 1.1337\n",
            "Época 22, Pérdida de entrenamiento: 0.3939, Precisión de entrenamiento: 0.9950\n",
            "Precisión en validación: 0.8929, Pérdida en validación: 1.0628\n",
            "Época 23, Pérdida de entrenamiento: 0.3555, Precisión de entrenamiento: 0.9950\n",
            "Precisión en validación: 0.8929, Pérdida en validación: 1.0000\n",
            "Época 24, Pérdida de entrenamiento: 0.3231, Precisión de entrenamiento: 1.0000\n",
            "Precisión en validación: 0.9286, Pérdida en validación: 0.9480\n",
            "Época 25, Pérdida de entrenamiento: 0.2910, Precisión de entrenamiento: 1.0000\n",
            "Precisión en validación: 0.9107, Pérdida en validación: 0.8780\n",
            "Respuesta predicha: tasa de fatalidad reciente cfr del 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar el modelo con una pregunta nueva\n",
        "pregunta_nueva = \"¿ Qué virus de la influenza se identificó en China en 2013?\" # bien\n",
        "pregunta_nueva = \"¿ Qué  virus de la influenza se descubrio en China en 2013?\" # bien\n",
        "pregunta_nueva = \"virus de la influenza en China en 2013?\" #\n",
        "pregunta_nueva = \"¿Cuál es el tratamiento para MERS-COV?\"\n",
        "respuesta_predicha = predecir_respuesta(model, pregunta_nueva)\n",
        "print(\"\\nPregunta:\", pregunta_nueva)\n",
        "print(\"Respuesta predicha:\", respuesta_predicha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYbB5rtvn4x2",
        "outputId": "d1e6d34c-60ba-4d11-d237-5d5c81212e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: ¿Cuál es el tratamiento para MERS-COV?\n",
            "Respuesta predicha: no hay un tratamiento específico para merscov al igual que la mayoría de las infecciones virales las opciones de tratamiento son de apoyo y sintomáticas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Una vez termiando el modelo anterior realicen lo siguinte:\n",
        "\n",
        "# TODO 1: Modificar el número de épocas (epochs).\n",
        "# Observa si al aumentar las épocas, la precisión en el conjunto de prueba sigue mejorando\n",
        "# o si llega un punto en el que el modelo ya no muestra mejoras.\n",
        "\n",
        "# TODO 2: Implementa \"early stopping\".\n",
        "# Define cuántas épocas sin mejora en la pérdida quieres permitir antes de detener el entrenamiento.\n",
        "# ¿Notas alguna diferencia en el tiempo de entrenamiento y la precisión final?\n",
        "\n",
        "# TODO 3: Ajusta la longitud de las secuencias (max_len).\n",
        "# ¿Produce una longitud de secuencia más larga o más corta un modelo que predice mejor las respuestas?\n",
        "\n",
        "# TODO 4: Cambia la función de activación en el modelo LSTM.\n",
        "# Prueba entre 'relu', 'tanh' e 'identity' y observa cuál captura mejor el estilo de respuesta.\n",
        "# ¿A qué se puede deber la diferencia entre las activaciones?\n",
        "\n",
        "# TODO 5: Reduce el tamaño de hidden_size.\n",
        "# Observa si una capa oculta más pequeña sigue generando respuestas coherentes.\n",
        "# ¿En qué afecta a la precisión del modelo?\n",
        "\n",
        "# TODO 6: Añade una capa de dropout en el modelo.\n",
        "# ¿Cómo impacta el dropout en la precisión y generalización? Observa si reduce la repetición en respuestas.\n",
        "\n",
        "# TODO 7: Cambia la frase inicial para predicciones (pregunta_nueva).\n",
        "# ¿El cambio en la frase inicial afecta el estilo de las respuestas?\n",
        "\n",
        "# TODO 8: Ajusta el batch_size.\n",
        "# Observa cómo impacta el tamaño del batch en la velocidad de entrenamiento y la precisión.\n",
        "\n",
        "# TODO 9: Cambia la longitud de la respuesta generada (longitud en predecir_respuesta).\n",
        "# ¿El modelo puede mantener coherencia en respuestas largas?\n",
        "\n",
        "# TODO 10: Agrega gráficos de pérdida por época.\n",
        "# Visualiza la pérdida para observar tendencias y detectar posibles problemas en el entrenamiento.\n"
      ],
      "metadata": {
        "id": "uzDVOl0fyRC5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}