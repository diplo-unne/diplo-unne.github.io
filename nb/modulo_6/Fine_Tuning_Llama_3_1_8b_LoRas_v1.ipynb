{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning de Llama-3.1 8B\n",
        "\n",
        "Aqui tratamos de afinar el modelo Llama-3.1 8B utilizando el conjunto de datos Alpaca de Yahma, disponible en Hugging Face. Este conjunto de datos es una versión filtrada y limpia del conjunto original de Alpaca, creado por investigadores de la Universidad de Stanford.\n",
        "\n",
        "**Tools:**\n",
        "* El conjunto de datos Alpaca es de propósito general y contiene aproximadamente 52,000 pares de instrucciones y respuestas, abarcando una amplia gama de temas y tareas como preguntas generales, escritura, resolución de problemas y explicaciones. No está enfocado en un área específica, sino que tiene como objetivo mejorar la capacidad del modelo para seguir instrucciones y generar respuestas coherentes en diversos contextos\n",
        "\n",
        "* La librería **Unsloth**, lo que permite entrenar el modelo de manera más rápida y eficiente en un entorno como Google Colab.\n",
        "\n",
        "**Objetivos:**\n",
        "\n",
        "1. **Instalación de Unsloth:**\n",
        "   - Se instala y actualiza la librería Unsloth, esencial para acelerar el entrenamiento de modelos grandes.\n",
        "   - Unsloth proporciona optimizaciones que permiten entrenar modelos con menos uso de memoria y mayor velocidad.\n",
        "\n",
        "2. **Carga del modelo y tokenizador:**\n",
        "   - Se definen parámetros clave como la longitud máxima de secuencia y el tipo de datos (por ejemplo, Float16 o BFloat16).\n",
        "   - Se carga el modelo pre-entrenado Llama-3.1 8B y su tokenizador asociado utilizando la función proporcionada por Unsloth.\n",
        "   - Se habilita la opción de cargar el modelo en 4 bits para reducir el uso de memoria (cuantización en 4 bits).\n",
        "\n",
        "3. **Configuración de adaptadores LoRA:**\n",
        "   - Se aplican adaptadores **LoRA** (Low-Rank Adaptation) al modelo, lo que permite entrenar solo una pequeña parte de los parámetros (entre 1% y 10%), reduciendo significativamente el tiempo y recursos necesarios.\n",
        "   - Se configuran los hiperparámetros de LoRA, como el rango `r`, `lora_alpha` y `lora_dropout`, y se especifican los módulos del modelo que serán ajustados.\n",
        "\n",
        "4. **Preparación del conjunto de Datos:**\n",
        "   - Se utiliza el conjunto de datos **Alpaca** de Yahma, una versión filtrada y limpia del original, ideal para tareas de ajuste fino.\n",
        "   - Se define un formato de prompt personalizado que incluye una instrucción, una entrada opcional y un espacio para la respuesta.\n",
        "   - Se procesa el conjunto de datos para adaptarlo al formato requerido, asegurándose de añadir el token de fin de secuencia (**EOS_TOKEN**) para evitar generaciones infinitas durante la inferencia.\n",
        "\n",
        "5. **Entrenamiento del modelo:**\n",
        "   - Se configura el entrenador utilizando el **SFTTrainer** de la librería **TRL** (Training Reward Learning).\n",
        "   - Se establecen argumentos de entrenamiento como el tamaño de lote, pasos de calentamiento, tasa de aprendizaje, optimizador y otros hiperparámetros relevantes.\n",
        "   - Se inicia el proceso de entrenamiento, durante el cual se monitorean y registran estadísticas de memoria y tiempo para evaluar el rendimiento y eficiencia.\n",
        "\n",
        "6. **Generación de respuestas (Inferencia):**\n",
        "   - Se habilita el modo de inferencia optimizada proporcionado por Unsloth para mejorar la velocidad de generación.\n",
        "   - Se proporciona un ejemplo de instrucción y entrada, y se genera una respuesta utilizando el modelo ajustado.\n",
        "   - Se demuestra cómo el modelo puede ser utilizado para generar respuestas coherentes y relevantes basadas en las instrucciones dadas.\n",
        "\n",
        "7. **Guardado del modelo ajustado:**\n",
        "   - Se guardan los adaptadores LoRA resultantes del entrenamiento, lo que permite cargar y utilizar el modelo ajustado en futuras sesiones sin necesidad de reentrenamiento.\n",
        "   - Se ofrece la opción de cargar estos adaptadores para realizar inferencia en otros entornos o compartirlos con la comunidad.\n",
        "\n",
        "\n",
        "\n",
        "- **Optimizados:**\n",
        "  - Se aprovechan las optimizaciones de Unsloth, como la cuantización en 4 bits y el uso de gradient checkpointing, para reducir el uso de memoria y acelerar el entrenamiento.\n",
        "  - Se ajustan dinámicamente parámetros como el tipo de datos (`fp16` o `bf16`) en función del hardware disponible, garantizando compatibilidad y rendimiento óptimo.\n"
      ],
      "metadata": {
        "id": "eRpDxXD3k1ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de Unsloth\n",
        "!pip install unsloth\n",
        "# Instalamos la última versión de Unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZfMgLW-hzgz",
        "outputId": "7369952c-e5f5-4c77-ecb8-71d11a4b86e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.10/dist-packages (2024.11.7)\n",
            "Found existing installation: unsloth 2024.11.7\n",
            "Uninstalling unsloth-2024.11.7:\n",
            "  Successfully uninstalled unsloth-2024.11.7\n",
            "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-h8vzzajg/unsloth_49a8c48aa12841629d81fd56a4412c1c\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-h8vzzajg/unsloth_49a8c48aa12841629d81fd56a4412c1c\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit f26d4e739ed507de7a9088da53d10fd02f58d160\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: unsloth-zoo>=2024.11.1 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.14)\n",
            "Requirement already satisfied: transformers>=4.46.1 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.46.2)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.26.2)\n",
            "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.8)\n",
            "Requirement already satisfied: bitsandbytes>=0.43.3 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.20.3)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.0)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.1.1)\n",
            "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.12.1)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.13.2)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2024.11.7-py3-none-any.whl size=163138 sha256=8f49fa0959e114834ad2415d799bd7d1811a5d814221ba83e5b2f4d7a07b9ee2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6jzzz37x/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
            "Successfully built unsloth\n",
            "Installing collected packages: unsloth\n",
            "Successfully installed unsloth-2024.11.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Definición de variables globales\n",
        "MAX_SEQ_LENGTH = 2048  # Longitud máxima de secuencia. Se puede ajustar según necesidad.\n",
        "DTYPE = None  # Tipo de datos. None para detección automática, Float16 para Tesla T4/V100, BFloat16 para Ampere+.\n",
        "LOAD_IN_4BIT = True  # Cargar el modelo en 4 bits para reducir uso de memoria.\n",
        "\n",
        "# Lista de modelos pre-cuantizados en 4 bits soportados\n",
        "FOURBIT_MODELS = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\",\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",\n",
        "    # Más modelos disponibles en https://huggingface.co/unsloth\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y89eePNih2uL",
        "outputId": "d2cc3eaf-5263-4f32-96ad-ced75d8da519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, max_seq_length, dtype, load_in_4bit):\n",
        "    #Carga el modelo y el tokenizador usando FastLanguageModel de Unsloth.\n",
        "    # model_name: Nombre del modelo a cargar.\n",
        "    # max_seq_length: Longitud máxima de secuencia.\n",
        "    # dtype: Tipo de dato (None para detección automática).\n",
        "    # load_in_4bit: Booleano para cargar en 4 bits.\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=dtype,\n",
        "        load_in_4bit=load_in_4bit,\n",
        "        # token=\"hf_...\",  # Usar si se requieren tokens de autenticación\n",
        "    )\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "vWqTSb1-h41q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_lora(model, r=16, lora_alpha=16, lora_dropout=0, use_gc=\"unsloth\", random_state=3407):\n",
        "    \"\"\"\n",
        "    Configura los adaptadores LoRA para el modelo.\n",
        "\n",
        "    Parámetros:\n",
        "    - model: El modelo al que se aplicarán los adaptadores.\n",
        "    - r: Rango de LoRA (sugerido 8, 16, 32, 64, 128).\n",
        "    - lora_alpha: Hiperparámetro de LoRA.\n",
        "    - lora_dropout: Dropout para LoRA (0 es óptimo para esta configuración).\n",
        "    - use_gc: Uso de gradient checkpointing (\"unsloth\" para optimización).\n",
        "    - random_state: Semilla para reproducibilidad.\n",
        "\n",
        "    Retorna:\n",
        "    - model: Modelo con los adaptadores LoRA configurados.\n",
        "    \"\"\"\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=r,\n",
        "        target_modules=target_modules,\n",
        "        lora_alpha=lora_alpha,\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\",  # \"none\" es óptimo para esta configuración\n",
        "        use_gradient_checkpointing=use_gc,\n",
        "        random_state=random_state,\n",
        "        use_rslora=False,  # No usamos Rank Stabilized LoRA en este caso\n",
        "        loftq_config=None,  # No usamos LoftQ\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "7BG6Sv0wh7LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(tokenizer, dataset_name=\"yahma/alpaca-cleaned\", split=\"train\"):\n",
        "    \"\"\"\n",
        "    Prepara el conjunto de datos para el entrenamiento con flexibilidad en las claves de entrada.\n",
        "\n",
        "    Parámetros:\n",
        "    - tokenizer: El tokenizador del modelo.\n",
        "    - dataset_name: Nombre del conjunto de datos a utilizar.\n",
        "    - split: División del conjunto de datos a cargar.\n",
        "\n",
        "    Retorna:\n",
        "    - dataset: Conjunto de datos procesado y tokenizado.\n",
        "    \"\"\"\n",
        "    # Definimos el formato del prompt\n",
        "    default_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "    poem_prompt = \"\"\"Below is a poem. Write a response that analyzes or interprets its meaning.\n",
        "\n",
        "### Poem:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "    EOS_TOKEN = tokenizer.eos_token  # Añadimos el token EOS\n",
        "\n",
        "    def formatting_prompts_func(examples):\n",
        "        # Verificar si las claves estándar existen\n",
        "        if \"instruction\" in examples and \"input\" in examples and \"output\" in examples:\n",
        "            instructions = examples[\"instruction\"]\n",
        "            inputs = examples[\"input\"]\n",
        "            outputs = examples[\"output\"]\n",
        "            texts = [\n",
        "                default_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
        "                for instruction, input_text, output in zip(instructions, inputs, outputs)\n",
        "            ]\n",
        "        else:\n",
        "            # Para el dataset de poemas con solo 'poem'\n",
        "            poems = examples[\"poem\"]\n",
        "            texts = [\n",
        "                poem_prompt.format(poem, \"\") + EOS_TOKEN for poem in poems\n",
        "            ]\n",
        "        return {\"text\": texts}\n",
        "\n",
        "    # Cargamos y procesamos el conjunto de datos\n",
        "    dataset = load_dataset(dataset_name, split=split)\n",
        "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "S12aOjgF5uyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_dataset_examples(dataset_name=\"xaviviro/FEDERICO-GARCIA-LORCA-canciones-poemas-romances\", split=\"train\", num_examples=5):\n",
        "    \"\"\"\n",
        "    Imprime ejemplos de un dataset, adaptándose automáticamente a las claves disponibles.\n",
        "\n",
        "    Parámetros:\n",
        "    - dataset_name: Nombre del dataset en Hugging Face.\n",
        "    - split: División del dataset a cargar (e.g., 'train').\n",
        "    - num_examples: Número de ejemplos a mostrar.\n",
        "    \"\"\"\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    # Cargar el dataset\n",
        "    dataset = load_dataset(dataset_name, split=split)\n",
        "\n",
        "    # Mostrar ejemplos\n",
        "    print(f\"Mostrando {num_examples} ejemplos del dataset '{dataset_name}' ({split} split):\\n\")\n",
        "    for i in range(num_examples):\n",
        "        example = dataset[i]\n",
        "        print(f\"Ejemplo {i+1}:\")\n",
        "\n",
        "        # Adaptar según las claves disponibles\n",
        "        if \"instruction\" in example and \"input\" in example and \"output\" in example:\n",
        "            print(\"Instruction:\", example[\"instruction\"])\n",
        "            print(\"Input:\", example[\"input\"])\n",
        "            print(\"Output:\", example[\"output\"])\n",
        "        elif \"poem\" in example:\n",
        "            print(\"Poem:\", example[\"poem\"])\n",
        "        else:\n",
        "            print(\"Formato desconocido. Claves disponibles:\", example.keys())\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Llamada de ejemplo con el dataset de Alpaca\n",
        "#print_dataset_examples(dataset_name=\"bertin-project/alpaca-spanish\")\n",
        "\n",
        "# Llamada de ejemplo con el dataset de poemas\n",
        "#print_dataset_examples(dataset_name=\"xaviviro/FEDERICO-GARCIA-LORCA-canciones-poemas-romances\")\n"
      ],
      "metadata": {
        "id": "noH_UGgPyHB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, tokenizer, dataset, max_seq_length):\n",
        "    \"\"\"\n",
        "    Entrena el modelo utilizando SFTTrainer.\n",
        "\n",
        "    Parámetros:\n",
        "    - model: El modelo a entrenar.\n",
        "    - tokenizer: El tokenizador asociado.\n",
        "    - dataset: Conjunto de datos para entrenamiento.\n",
        "    - max_seq_length: Longitud máxima de secuencia.\n",
        "\n",
        "    Retorna:\n",
        "    - trainer_stats: Estadísticas del entrenamiento.\n",
        "    \"\"\"\n",
        "    # Determinamos si usar fp16 o bf16\n",
        "    fp16 = not is_bfloat16_supported()\n",
        "    bf16 = is_bfloat16_supported()\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        # num_train_epochs=1,  # Descomentar para un entrenamiento completo\n",
        "        max_steps=60,  # Limitamos a 60 pasos para acelerar pruebas\n",
        "        learning_rate=2e-4,\n",
        "        fp16=fp16,\n",
        "        bf16=bf16,\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_num_proc=2,\n",
        "        packing=False,  # Puede acelerar el entrenamiento para secuencias cortas\n",
        "        args=training_args,\n",
        "    )\n",
        "\n",
        "    # Obtenemos estadísticas iniciales de la GPU\n",
        "    gpu_stats = torch.cuda.get_device_properties(0)\n",
        "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 3)\n",
        "    max_memory = round(gpu_stats.total_memory / 1024**3, 3)\n",
        "    print(f\"GPU = {gpu_stats.name}. Memoria máxima = {max_memory} GB.\")\n",
        "    print(f\"{start_gpu_memory} GB de memoria reservada al inicio.\")\n",
        "\n",
        "    # Entrenamos el modelo\n",
        "    trainer_stats = trainer.train()\n",
        "\n",
        "    # Estadísticas de memoria y tiempo tras el entrenamiento\n",
        "    used_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 3)\n",
        "    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "    used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "    lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "    print(f\"{trainer_stats.metrics['train_runtime']} segundos utilizados para el entrenamiento.\")\n",
        "    print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutos en total.\")\n",
        "    print(f\"Memoria máxima reservada = {used_memory} GB.\")\n",
        "    print(f\"Memoria usada para entrenamiento = {used_memory_for_lora} GB.\")\n",
        "    print(f\"Porcentaje de memoria usada = {used_percentage} %.\")\n",
        "    print(f\"Porcentaje de memoria usada para entrenamiento = {lora_percentage} %.\")\n",
        "\n",
        "    return trainer_stats\n"
      ],
      "metadata": {
        "id": "5V1VeBdsiBIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(model, tokenizer, instruction, input_text=\"\", max_new_tokens=64):\n",
        "    \"\"\"\n",
        "    Genera una respuesta dada una instrucción y un input opcional.\n",
        "\n",
        "    Parámetros:\n",
        "    - model: El modelo para generación.\n",
        "    - tokenizer: El tokenizador asociado.\n",
        "    - instruction: Instrucción para el modelo.\n",
        "    - input_text: Texto de entrada adicional.\n",
        "    - max_new_tokens: Número máximo de tokens a generar.\n",
        "\n",
        "    Retorna:\n",
        "    - response: Texto generado por el modelo.\n",
        "    \"\"\"\n",
        "    # Formato del prompt\n",
        "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "    # Preparamos la entrada\n",
        "    inputs = tokenizer(\n",
        "        [alpaca_prompt.format(instruction, input_text, \"\")],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Habilitamos la inferencia optimizada\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    # Generamos la respuesta\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
        "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "IDbdLDwdiDLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, tokenizer, save_directory=\"lora_model\"):\n",
        "    #Guarda los adaptadores LoRA del modelo.\n",
        "    # model: El modelo entrenado con adaptadores LoRA.\n",
        "    # tokenizer: El tokenizador asociado al modelo.\n",
        "    # save_directory: Directorio donde se guardará el modelo.\n",
        "\n",
        "    model.save_pretrained(save_directory)\n",
        "    tokenizer.save_pretrained(save_directory)\n",
        "    print(f\"Modelo y tokenizador guardados en {save_directory}\")\n"
      ],
      "metadata": {
        "id": "B6ao_FqhiIOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga del modelo y tokenizador\n",
        "model_name = \"unsloth/Meta-Llama-3.1-8B\"  # Puede cambiarse por otro modelo de FOURBIT_MODELS\n",
        "model, tokenizer = load_model(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=DTYPE,\n",
        "    load_in_4bit=LOAD_IN_4BIT\n",
        ")\n",
        "\n",
        "# Configuración de adaptadores LoRA\n",
        "model = configure_lora(model)\n",
        "\n",
        "# Preparación de los datos\n",
        "dataset = prepare_data(tokenizer)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "trainer_stats = train_model(model, tokenizer, dataset, MAX_SEQ_LENGTH)\n",
        "\n",
        "# Ejemplo de inferencia\n",
        "instruction = \"Continúa la secuencia de Fibonacci.\"\n",
        "input_text = \"1, 1, 2, 3, 5, 8\"\n",
        "response = generate_response(model, tokenizer, instruction, input_text)\n",
        "print(\"Respuesta generada:\")\n",
        "print(response)\n",
        "\n",
        "# Guardamos el modelo entrenado\n",
        "save_model(model, tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3defcedaa70849908aad34c50a7b0eba",
            "383ba24d78dc46f5aa3676639d2e9559",
            "90fc5e0c98ee4321997824cfe1bdb753",
            "c9cf9fad36a745e682106be281aa13b1",
            "da24bc52a2e445c08c2bea7b3358afb1",
            "0a1b1b7f0b0e4d9cbc7761bea0b0928a",
            "6b6b71d4f49e4e849e5e14aa669b3831",
            "a6f4249222de4856825b876f1341ea26",
            "71adab62f59646a4bede0ddc51d85785",
            "e35463a582164fa3a53df43138a2c4f1",
            "b6ce075968a946e2a38c044d9e77c11d",
            "e0a5d0b34dba4c38a2c02941566ca10b",
            "f969b4756a2745f6818ef7b5ab000e44",
            "d3f765ce8d9b4ce2bb23a3dfb9edf3ef",
            "4067a9f598454d418c9ea80e86305bff",
            "564ef94b41a84088aebdbbe0046a0b25",
            "0270fdf45f724daa8c5016c571290c63",
            "3e4ce1d8e0c146f9ae926acc121984df",
            "da27e64a4d324db0a49cd5a971044d23",
            "1d1e33bfdcad40aaaa1dde8323c4d88e",
            "c80bb5df78774fa08ffcd0a542b441a9",
            "cace669dcc2849dfb9584b2fd7b947f6"
          ]
        },
        "id": "FNBBv2WBiLsk",
        "outputId": "afb8f197-2537-4876-9274-d583dfd83a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.11.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3defcedaa70849908aad34c50a7b0eba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/51760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0a5d0b34dba4c38a2c02941566ca10b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Memoria máxima = 14.748 GB.\n",
            "5.984 GB de memoria reservada al inicio.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 51,760 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 08:03, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.586800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.115200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.672900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.863000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.678900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.490100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.080900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.144400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.122400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.904600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.997500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.919200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.079600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.900200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.864000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.992400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.338500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.033300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.872000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.923800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.989000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.991000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.995600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.072400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.045900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.040700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.926400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.927400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.852900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.861800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.874200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.009500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.862300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.999800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.879800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.819600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.733800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.060900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.180900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.901500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.968500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.878900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.911700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.969300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.941400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.816200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.240000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.844000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.059500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.038900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.899000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.006200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.174900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.794500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.999200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.880400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.771300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.837300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.885300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "493.4542 segundos utilizados para el entrenamiento.\n",
            "8.22 minutos en total.\n",
            "Memoria máxima reservada = 7.924 GB.\n",
            "Memoria usada para entrenamiento = 1.94 GB.\n",
            "Porcentaje de memoria usada = 53.729 %.\n",
            "Porcentaje de memoria usada para entrenamiento = 13.154 %.\n",
            "Respuesta generada:\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Continúa la secuencia de Fibonacci.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765\n",
            "Modelo y tokenizador guardados en lora_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Esto deberia vaciar la memmoria de la GPU, pero google colab mantiene algunas caches ocultas\n",
        "import torch\n",
        "\n",
        "# Elimina todos los tensores de la GPU\n",
        "torch.cuda.empty_cache()\n",
        "# Elimina todos los tensores de la GPU\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tXxkWHCF0D0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de inferencia\n",
        "instruction = \"Continúa la secuencia de Fibonacci.\"\n",
        "input_text = \"1, 1, 2, 3, 5, 8, 13\"\n",
        "response = generate_response(model, tokenizer, instruction, input_text)\n",
        "print(\"Respuesta generada:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVPRE68cuzqR",
        "outputId": "d60c132a-5b10-45c7-ebef-77e1ebd78e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta generada:\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Continúa la secuencia de Fibonacci.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8, 13\n",
            "\n",
            "### Response:\n",
            "Continuando la secuencia de Fibonacci, los siguientes números serían: 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO 1: Utiliza diferentes dataset en español:\n",
        "# bertin-project/alpaca-spanish\n",
        "# xaviviro/FEDERICO-GARCIA-LORCA-canciones-poemas-romances\n",
        "\n",
        "### TODO 2: Cambiar el tamaño del lote si la memoria de la GPU lo permite.\n",
        "\n",
        "### TODO 3: Ajustar los pasos de acumulación de gradientes según la capacidad de hardware.\n",
        "\n",
        "### TODO 4: Modificar warmup_steps para experimentos con diferentes curvas de aprendizaje.\n",
        "\n",
        "### TODO 5: Cambiar num_train_epochs para entrenamientos completos.\n",
        "\n",
        "### TODO 6: Ajustar max_steps para pruebas rápidas o entrenamientos largos.\n",
        "\n",
        "### TODO 7: Modificar learning_rate según el optimizador o tamaño del modelo.\n",
        "\n",
        "### TODO 8: Ajustar logging_steps para monitorear el entrenamiento con mayor o menor frecuencia.\n",
        "\n",
        "### TODO 9: Cambiar optim a un optimizador diferente si es necesario.\n",
        "\n",
        "### TODO 10: Ajustar weight_decay para diferentes niveles de regularización.\n",
        "\n",
        "### TODO 11: Cambiar lr_scheduler_type para probar diferentes estrategias de decaimiento.\n",
        "\n",
        "### TODO 12: Establecer seed para reproducibilidad si se requiere.\n",
        "\n",
        "### TODO 13: Cambiar output_dir para guardar los resultados en otro directorio.\n",
        "\n",
        "### TODO 14: Ajustar max_seq_length según la longitud esperada de las secuencias en el dataset.\n",
        "\n",
        "### TODO 15: Revisar dataset_num_proc para paralelismo óptimo en la tokenización.\n"
      ],
      "metadata": {
        "id": "-zkofSIZFjns"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3defcedaa70849908aad34c50a7b0eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_383ba24d78dc46f5aa3676639d2e9559",
              "IPY_MODEL_90fc5e0c98ee4321997824cfe1bdb753",
              "IPY_MODEL_c9cf9fad36a745e682106be281aa13b1"
            ],
            "layout": "IPY_MODEL_da24bc52a2e445c08c2bea7b3358afb1"
          }
        },
        "383ba24d78dc46f5aa3676639d2e9559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a1b1b7f0b0e4d9cbc7761bea0b0928a",
            "placeholder": "​",
            "style": "IPY_MODEL_6b6b71d4f49e4e849e5e14aa669b3831",
            "value": "Map: 100%"
          }
        },
        "90fc5e0c98ee4321997824cfe1bdb753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6f4249222de4856825b876f1341ea26",
            "max": 51760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71adab62f59646a4bede0ddc51d85785",
            "value": 51760
          }
        },
        "c9cf9fad36a745e682106be281aa13b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e35463a582164fa3a53df43138a2c4f1",
            "placeholder": "​",
            "style": "IPY_MODEL_b6ce075968a946e2a38c044d9e77c11d",
            "value": " 51760/51760 [00:00&lt;00:00, 65070.40 examples/s]"
          }
        },
        "da24bc52a2e445c08c2bea7b3358afb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a1b1b7f0b0e4d9cbc7761bea0b0928a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b6b71d4f49e4e849e5e14aa669b3831": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6f4249222de4856825b876f1341ea26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71adab62f59646a4bede0ddc51d85785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e35463a582164fa3a53df43138a2c4f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6ce075968a946e2a38c044d9e77c11d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0a5d0b34dba4c38a2c02941566ca10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f969b4756a2745f6818ef7b5ab000e44",
              "IPY_MODEL_d3f765ce8d9b4ce2bb23a3dfb9edf3ef",
              "IPY_MODEL_4067a9f598454d418c9ea80e86305bff"
            ],
            "layout": "IPY_MODEL_564ef94b41a84088aebdbbe0046a0b25"
          }
        },
        "f969b4756a2745f6818ef7b5ab000e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0270fdf45f724daa8c5016c571290c63",
            "placeholder": "​",
            "style": "IPY_MODEL_3e4ce1d8e0c146f9ae926acc121984df",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "d3f765ce8d9b4ce2bb23a3dfb9edf3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da27e64a4d324db0a49cd5a971044d23",
            "max": 51760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d1e33bfdcad40aaaa1dde8323c4d88e",
            "value": 51760
          }
        },
        "4067a9f598454d418c9ea80e86305bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c80bb5df78774fa08ffcd0a542b441a9",
            "placeholder": "​",
            "style": "IPY_MODEL_cace669dcc2849dfb9584b2fd7b947f6",
            "value": " 51760/51760 [00:38&lt;00:00, 1255.42 examples/s]"
          }
        },
        "564ef94b41a84088aebdbbe0046a0b25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0270fdf45f724daa8c5016c571290c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e4ce1d8e0c146f9ae926acc121984df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da27e64a4d324db0a49cd5a971044d23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d1e33bfdcad40aaaa1dde8323c4d88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c80bb5df78774fa08ffcd0a542b441a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cace669dcc2849dfb9584b2fd7b947f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}