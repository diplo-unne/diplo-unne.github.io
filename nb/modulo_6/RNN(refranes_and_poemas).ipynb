{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Módulo 6 - Aprendizaje profundo\n",
        "## Clase 3: RNN"
      ],
      "metadata": {
        "id": "nFhqfWBfrURx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parámetros y Hiperparámetros  modelo RNN\n",
        "\n",
        "A continuación se describen los principales parámetros y los hiperparámetros que se pueden ajustar en el modelo LSTM (RNN):\n",
        "\n",
        "#### 1. **vocab_size**\n",
        "- **Descripción**: El tamaño del vocabulario utilizado por el modelo. Representa el número de caracteres únicos que el modelo puede reconocer y generar.\n",
        "- **Importancia**: Cuanto más grande sea el vocabulario, más diverso puede ser el texto generado. Sin embargo, vocabularios más grandes pueden aumentar la complejidad del modelo.\n",
        "\n",
        "#### 2. **embed_size**\n",
        "- **Descripción**: Tamaño de la capa de embeddings. Los embeddings transforman los caracteres en vectores de características que capturan relaciones entre ellos.\n",
        "- **Importancia**: Un tamaño mayor permite representar más información en cada vector de caracteres. Sin embargo, tamaños demasiado grandes pueden aumentar el tiempo de entrenamiento.\n",
        "\n",
        "#### 3. **hidden_size**\n",
        "- **Descripción**: El número de unidades en las capas ocultas (hidden layers) de la LSTM.\n",
        "- **Importancia**: Determina la capacidad de la red para aprender patrones complejos. Un valor alto permite que el modelo capture más detalles, pero puede llevar a tiempos de entrenamiento más largos.\n",
        "\n",
        "#### 4. **num_layers**\n",
        "- **Descripción**: El número de capas LSTM apiladas en el modelo.\n",
        "- **Importancia**: Aumentar el número de capas puede permitir que el modelo capture estructuras más complejas, pero también puede llevar a problemas como el desvanecimiento del gradiente (vanishing gradients) o sobreajuste (overfitting) si no se maneja adecuadamente.\n",
        "\n",
        "#### 5. **dropout**\n",
        "- **Descripción**: Tasa de desactivación de neuronas durante el entrenamiento para prevenir el sobreajuste.\n",
        "- **Importancia**: Valores más altos de dropout (ej. 0.3 a 0.5) pueden ayudar a reducir el sobreajuste, pero tasas muy altas pueden afectar el rendimiento del modelo.\n",
        "\n",
        "#### 6. **activation**\n",
        "- **Descripción**: La función de activación que se aplica después de la LSTM. Puede ser `relu`, `tanh`, o ninguna (si se especifica `Identity`).\n",
        "- **Importancia**: `relu` es una función común en redes neuronales debido a su eficiencia en resolver problemas de gradiente, mientras que `tanh` puede ser más adecuada para capturar relaciones no lineales en datos secuenciales como el texto.\n",
        "\n",
        "#### 7. **batch_size**\n",
        "- **Descripción**: Número de ejemplos que se pasan al modelo antes de actualizar los parámetros.\n",
        "- **Importancia**: Batch sizes más grandes pueden acelerar el entrenamiento al aprovechar mejor la paralelización, mientras que batch sizes más pequeños pueden ofrecer mayor precisión en la actualización de los parámetros.\n",
        "\n",
        "#### 8. **epochs**\n",
        "- **Descripción**: Número de veces que el conjunto de datos completo pasa por la red durante el entrenamiento.\n",
        "- **Importancia**: Más épocas permiten al modelo aprender mejor los patrones, pero demasiadas épocas pueden llevar a sobreajuste.\n",
        "\n",
        "#### 9. **learning_rate**\n",
        "- **Descripción**: Tasa de aprendizaje del modelo, que determina el tamaño de los pasos que el optimizador da durante el ajuste de los parámetros.\n",
        "- **Importancia**: Un valor de tasa de aprendizaje bajo puede hacer que el entrenamiento sea muy lento, mientras que un valor demasiado alto puede hacer que el modelo no converja correctamente.\n"
      ],
      "metadata": {
        "id": "pe_tj8bOddXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Ejemplo 1\n",
        "\n",
        "En este ejemplo, se utiliza una Red Neuronal Recurrente (RNN) para aprender a generar texto basado en un conjunto de refranes en español.\n",
        "\n",
        "\n",
        "### Paso 1: Crear el dataset de refranes\n",
        "Inicia con una lista de refranes en español, que se prepara para que el modelo aprenda patrones de frases y palabras.\n",
        "\n",
        "### Paso 2: Preprocesamiento del texto\n",
        "Cada refrán se convierte en secuencias de palabras. Luego, se crean diccionarios que asignan a cada palabra un índice numérico, lo cual es esencial para el procesamiento en la RNN.\n",
        "\n",
        "### Paso 3: Convertir el texto en secuencias de índices\n",
        "El texto se transforma en secuencias numéricas, donde cada palabra se representa por un número. Esto ayuda al modelo a identificar y aprender patrones a nivel de palabras.\n",
        "\n",
        "### Paso 4: Definir el modelo RNN\n",
        "El modelo tiene tres componentes: una capa de embedding (convierte palabras en vectores), una capa LSTM (captura patrones en las secuencias) y una capa de salida (predice la siguiente palabra en la secuencia).\n",
        "\n",
        "### Paso 5: Función de pérdida y optimizador\n",
        "La función de pérdida mide el error del modelo, y el optimizador ajusta sus parámetros para mejorar su rendimiento en cada iteración.\n",
        "\n",
        "### Paso 6: Configuración del dispositivo\n",
        "Para mejorar la velocidad de entrenamiento, el modelo y los datos se transfieren a la GPU, si está disponible.\n",
        "\n",
        "### Paso 7: Entrenamiento del modelo\n",
        "El modelo se entrena en múltiples ciclos (épocas), donde aprende a mejorar sus predicciones en cada pasada de los datos. Se imprime la pérdida promedio para evaluar el aprendizaje del modelo.\n",
        "\n",
        "### Paso 8: Generación de texto a partir de una semilla\n",
        "Usando una frase inicial, el modelo genera una secuencia de palabras al predecir cada palabra siguiente en base a las anteriores. Así, se crean nuevas frases o refranes.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d4_xRJ3qM6Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo 1: Generación de texto con una RNN simple usando refranes en español a nivel de palabras\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Paso 1: Crear un dataset ampliado de refranes en español\n",
        "refranes = [\n",
        "    \"Al mal tiempo, buena cara.\",\n",
        "    \"Más vale tarde que nunca.\",\n",
        "    \"No por mucho madrugar amanece más temprano.\",\n",
        "    \"A caballo regalado no se le mira el diente.\",\n",
        "    \"El que guarda su código, siempre encuentra respaldo.\",\n",
        "    \"El que mucho abarca poco aprieta.\",\n",
        "    \"En casa de herrero, cuchillo de palo.\",\n",
        "    \"A quien madruga, Dios le ayuda.\",\n",
        "    \"No hay mal que por bien no venga.\",\n",
        "    \"Ojos que no ven, corazón que no siente.\",\n",
        "    \"Más vale pájaro en mano que cientos volando.\",\n",
        "    \"No por mucho entrenar se logra generalizar.\",\n",
        "    \"A palabras necias, oídos sordos.\",\n",
        "    \"¿Tu GPU está al 100%? Mejor baja el batch antes de la combustión.\",\n",
        "    \"Dime cómo entrenas y te diré cómo predices.\",\n",
        "    \"El hábito no hace al monje.\",\n",
        "    \"Dime con quién andas y te diré quién eres.\",\n",
        "    \"Más sabe el diablo por viejo que por diablo.\",\n",
        "    \"En boca cerrada no entran moscas.\",\n",
        "    \"En las clases de ciencia de datos, el que lee a Bishop, hallara la respuesta.\",\n",
        "    \"Cría cuervos y te sacarán los ojos.\",\n",
        "    \"A buen entendedor, pocas palabras bastan.\",\n",
        "    \"Perro que ladra no muerde.\",\n",
        "    \"No hay peor ciego que el que no quiere ver.\",\n",
        "    \"Al que madruga, Dios le ayuda.\",\n",
        "    \"Al estudiante que duda, su código le ayuda.\",\n",
        "    \"Más vale un buen 'debug' que diez errores ocultos.\",\n",
        "    \"Quien mucho 'importa', al final poco encuentra.\",\n",
        "    \"En las clases de ciencia de datos, el que pregunta mucho, aprende más.\",\n",
        "    \"Cuando el código falla, todos dicen: ¡era el dataset!\",\n",
        "    \"No hay peor frustración que ver el error en la última ejecución.\",\n",
        "    \"A los datos duros, modelo con más parámetros.\",\n",
        "    \"Si te quedas sin RAM,, mejor ponte a muestrear.\",\n",
        "    \"Alumno que no entrena, no mejora su red.\",\n",
        "    \"Divide y vencerás, dijo el que hacía cross-validation.\"\n",
        "]\n",
        "\n",
        "# Paso 2: Preprocesamiento del texto a nivel de palabras\n",
        "texto = ' '.join(refranes).lower()\n",
        "palabras = texto.split()\n",
        "\n",
        "# Crear un conjunto de palabras únicas y definir el tamaño del vocabulario\n",
        "palabras_unicas = sorted(list(set(palabras)))\n",
        "vocab_size = len(palabras_unicas)\n",
        "\n",
        "# Crear diccionarios para mapear palabras a índices y viceversa\n",
        "word_to_idx = {word: i for i, word in enumerate(palabras_unicas)}\n",
        "idx_to_word = {i: word for i, word in enumerate(palabras_unicas)}\n",
        "\n",
        "print(f\"Total de palabras únicas (vocab_size): {vocab_size}\")\n",
        "\n",
        "# RECORDAR VAMOS A PREDECIR LA SIGUIENTE PALABRA DEL REFRAN\n",
        "\n",
        "# Paso 3: Convertir el texto a secuencias de índices\n",
        "texto_idx = [word_to_idx[word] for word in palabras]\n",
        "seq_length = 5\n",
        "step = 1 #  serie continua y solapada\n",
        "\n",
        "# Crear secuencias de entrada y sus palabras siguientes\n",
        "sequences = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(0, len(texto_idx) - seq_length, step):\n",
        "    sequences.append(texto_idx[i: i + seq_length])\n",
        "    next_words.append(texto_idx[i + seq_length])\n",
        "\n",
        "print(f\"Número de secuencias generadas: {len(sequences)}\")\n",
        "\n",
        "# Convertir secuencias a tensores de PyTorch int64\n",
        "X = torch.tensor(sequences, dtype=torch.long)\n",
        "y = torch.tensor(next_words, dtype=torch.long)\n",
        "\n",
        "# Ejemplo: seq_length = 4\n",
        "# X: [\"Perro\", \"que\", \"ladra\", \"no\"]   =   [12, 36, 38, 50]\n",
        "# y: [\"muerde\"]                        =   [14]\n",
        "\n",
        "# Dividir el dataset en entrenamiento y validación\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear DataLoader para entrenamiento y validación\n",
        "batch_size = 16 # recuerden el seq_length entonces tendremos (16, 5)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Tamaño de X: {X.shape}\")\n",
        "print(f\"Tamaño de y: {y.shape}\")\n",
        "\n",
        "\n",
        "# Parámetros del modelo\n",
        "embed_size = 128  # dimensión del vector para cada palabra\n",
        "# ejemplo embed_size = 3 , estos son los valores pesos que se ajustaran\n",
        "#\"perro\"    = [0.8, 0.1, 0.5]\n",
        "#\"ladra\"    = [0.7, 0.2, 0.5]\n",
        "#\"cuchillo\" = [0.1, 0.8, 0.3]\n",
        "\n",
        "hidden_size = 256  # neuronas en la capa interna de la LSTM, son la \"memoria\" de la red\n",
        "# ejemplo hidden_size = 3 ,\n",
        "#[0.6, 0.5, 0.4], \"perro\", \"ladra\", \"cuchillo\"(es el ultimo pesa menos)\n",
        "\n",
        "\n",
        "# Paso 4: Definir la arquitectura del modelo RNN a nivel de palabras\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size # tamaño capa oculta\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size) #(palabras unicas, dimensión del vector palabra)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True) # I/O (batch_size, seq_length, embed_size)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size) # i: salida de la capa oculta (memoria), O: puntuaciones para cada palabra del vocabulario\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out[:, -1, :])  # bash, palabras(ultima palabra), embedding\n",
        "        return out, hidden # devolvemos la predicion y la memoria\n",
        "\n",
        "    def init_hidden(self, batch_size): # inicia estado oculto\n",
        "        return (torch.zeros(1, batch_size, self.hidden_size),\n",
        "                torch.zeros(1, batch_size, self.hidden_size)) # (h_0, c_0) corto y largo plazo\n",
        "\n",
        "model = SimpleRNN(vocab_size, embed_size, hidden_size)\n",
        "\n",
        "# Paso 5: Definir la función de pérdida y el optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Paso 6: Configuración del dispositivo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Entrenar el modelo usando DataLoader\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        hidden = model.init_hidden(X_batch.size(0)) # inicializo por bash\n",
        "        hidden = tuple([h.to(device) for h in hidden]) # Mueve el estado oculto (h_0, c_0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(X_batch, hidden)\n",
        "        loss = criterion(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Época {epoch + 1}/{num_epochs}, Pérdida promedio de entrenamiento: {avg_train_loss:.4f}\")\n",
        "\n",
        "# Paso 7: Función para generar texto a partir de una semilla\n",
        "def generar_texto(model, seed_text, length=20):\n",
        "    model.eval()\n",
        "    generated = seed_text\n",
        "    words = seed_text.lower().split()\n",
        "    hidden = model.init_hidden(1)\n",
        "    hidden = tuple([h.to(device) for h in hidden])\n",
        "\n",
        "    # Convertir las palabras de la semilla a índices\n",
        "    input_seq = torch.tensor([[word_to_idx.get(word, 0) for word in words]], dtype=torch.long).to(device)\n",
        "\n",
        "    for _ in range(length):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        prob = output.softmax(dim=1).data # recuerden recibimos las probabilidas asociadas a nuestro vocabulario\n",
        "        word_idx = torch.multinomial(prob, 1).item() # seleciona 1 de las palabras más relevantes\n",
        "        word = idx_to_word[word_idx]\n",
        "        generated += ' ' + word\n",
        "\n",
        "        # Actualizar la secuencia de entrada con la nueva palabra\n",
        "        input_seq = torch.tensor([[word_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated\n",
        "\n",
        "# Paso 8: Probar la generación de texto a partir de una semilla\n",
        "seed_text = \"Más vale pájaro en mano\"\n",
        "texto_generado = generar_texto(model, seed_text, length=5)\n",
        "\n",
        "print(\"\\nTexto generado:\\n\")\n",
        "print(texto_generado)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3qCcjq1O_fE",
        "outputId": "1a0d9448-a545-4fc5-8b22-1955181f9a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de palabras únicas (vocab_size): 162\n",
            "Número de secuencias generadas: 275\n",
            "Tamaño de X: torch.Size([275, 5])\n",
            "Tamaño de y: torch.Size([275])\n",
            "Época 10/50, Pérdida promedio de entrenamiento: 0.2970\n",
            "Época 20/50, Pérdida promedio de entrenamiento: 0.0513\n",
            "Época 30/50, Pérdida promedio de entrenamiento: 0.0288\n",
            "Época 40/50, Pérdida promedio de entrenamiento: 0.0221\n",
            "Época 50/50, Pérdida promedio de entrenamiento: 0.0178\n",
            "\n",
            "Texto generado:\n",
            "\n",
            "Más vale pájaro en mano que cientos no entrena, que\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 10: Probar la generación de texto a partir de una semilla\n",
        "seed_text = \"En casa de herrero,\"\n",
        "texto_generado = generar_texto(model, seed_text, length=5)\n",
        "\n",
        "print(\"\\nTexto generado:\\n\")\n",
        "print(texto_generado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPIqfoLsJaFL",
        "outputId": "07c12d29-69ad-4d06-e490-390891471ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Texto generado:\n",
            "\n",
            "En casa de herrero, cuchillo de palo. de palo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 1: Cambia el batch_size y num_epochs para optimizar el modelo\n",
        "\n",
        "# TODO 2: # Cambia el `learning_rate` en el optimizador.\n",
        "# ¿Qué observas cuando reduces o aumentas la tasa de aprendizaje? ¿Se estabiliza la pérdida más rápido?\n",
        "\n",
        "# TODO 3: Cambia `embed_size` y `hidden_size` en la definición del modelo.\n",
        "# ¿Cómo afecta el tamaño de embedding en el aprendizaje del modelo? ¿Y el tamaño de la capa oculta?\n",
        "\n",
        "# TODO 4:  Cambia `seq_length` en la generación de secuencias.\n",
        "# Pregunta: ¿La longitud de la secuencia afecta la coherencia del texto generado? ¿Por qué puede ser relevante?\n",
        "\n",
        "# TODO 5: Cambia el valor de `seed_text` en la función `generar_texto`.\n",
        "# ¿Cómo influye el texto inicial en las predicciones de la red? ¿La semilla afecta la coherencia del texto generado?\n",
        "\n",
        "# TODO 6: Agrega más refranes o crea variaciones de los refranes existentes.\n",
        "# ¿Crees que un conjunto de datos más grande ayudará al modelo a generar texto más coherente? ¿Por qué?\n"
      ],
      "metadata": {
        "id": "CWkDzKQubUHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Ejemplo 2\n",
        "\n",
        "Este ejemplo utiliza una Red Neuronal Recurrente (RNN) para generar texto basado en poemas en español, descargados de Project Gutenberg. El proceso sigue estos pasos:\n",
        "\n",
        "1. **Carga y Preparación de los Poemas**:\n",
        "   Se descarga el archivo de texto y se extraen los poemas de un rango de líneas específico. Cada poema se almacena en un diccionario para su fácil acceso y procesamiento.\n",
        "\n",
        "2. **Preprocesamiento del Texto**:\n",
        "   Todo el texto se convierte en una cadena única. Se crea un vocabulario de caracteres únicos, y se asigna un índice numérico a cada carácter. Esto permite que el modelo trabaje con representaciones numéricas.\n",
        "\n",
        "3. **Conversión a Secuencias de Entrenamiento**:\n",
        "   El texto se divide en secuencias de una longitud fija de caracteres, con el carácter siguiente como objetivo a predecir. Estas secuencias se convierten a tensores para PyTorch.\n",
        "\n",
        "4. **Definición del Modelo RNN (Modelo 1)**:\n",
        "   El modelo RNN incluye:\n",
        "   - Una capa de embeddings para convertir caracteres en vectores.\n",
        "   - Una capa LSTM para capturar patrones en secuencias.\n",
        "   - Una capa final para predecir el siguiente carácter.\n",
        "\n",
        "5. **Modelo Personalizable (Modelo 2)**:\n",
        "   Una versión del modelo permite cambiar parámetros como el tamaño de la capa oculta y el número de capas LSTM. Esto ayuda a comparar el impacto de diferentes configuraciones.\n",
        "\n",
        "6. **Entrenamiento del Modelo**:\n",
        "   Se entrena el modelo usando lotes de datos y un optimizador que ajusta sus parámetros. Se muestra la pérdida promedio en cada época, indicando el progreso del aprendizaje.\n",
        "\n",
        "7. **Generación de Texto con el Modelo**:\n",
        "   Partiendo de una frase inicial, el modelo genera texto carácter por carácter. Cada nuevo carácter generado se usa como entrada en el siguiente paso, extendiendo el texto a partir de la frase inicial."
      ],
      "metadata": {
        "id": "_ND1loh0RQxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poemas o cuentos en español:\n",
        "dataset de letras de poemas disponibles en Project Gutenberg.\n",
        "\n",
        "Ejemplo:\n",
        "Machado, Antonio, 1875-1939\n",
        "Páginas escogidas (Spanish) (as Author)\n",
        "Poesías completas (Spanish) (as Author)\n",
        "\n",
        "https://www.gutenberg.org/ebooks/68525\n"
      ],
      "metadata": {
        "id": "3SJVyyl6dbBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm  # Para mostrar bar\n",
        "import re  # Para expresiones regulares\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "TybBmV0VY8V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Esta celda se encarga de leer y procesa un archivo de texto para extraer poemas dentro de un rango de líneas dado.\n",
        "\n",
        "!wget https://www.gutenberg.org/cache/epub/68525/pg68525.txt -O machado.txt\n",
        "\n",
        "import re\n",
        "\n",
        "def procesar_poemas(ruta_archivo, inicio, fin):\n",
        "    # Paso 1: Leer todas las líneas del archivo\n",
        "    print(\"Leyendo el archivo...\")\n",
        "    with open(ruta_archivo, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Verificar si el rango es válido\n",
        "    if inicio < 0 or fin > len(lines):\n",
        "        raise ValueError(\"El rango de líneas es inválido para el archivo dado.\")\n",
        "\n",
        "    # Paso 2: Extraer solo las líneas dentro del rango especificado\n",
        "    print(f\"Extrayendo líneas desde {inicio} hasta {fin}...\")\n",
        "    poemas_lineas = lines[inicio:fin]\n",
        "\n",
        "    # Crear un diccionario para almacenar los títulos y contenido de los poemas\n",
        "    poemas_dict = {}\n",
        "\n",
        "    # Variables temporales para el título actual y el contenido del poema\n",
        "    titulo_actual = None\n",
        "    poema_actual = []\n",
        "\n",
        "    # Paso 3: Procesar cada línea dentro del rango para extraer los títulos y contenido\n",
        "    print(\"Procesando líneas para extraer títulos y poemas...\")\n",
        "    for i in range(len(poemas_lineas)):\n",
        "        linea = poemas_lineas[i].strip()\n",
        "\n",
        "        # Detectar si la línea contiene un número romano (presumiblemente indica un nuevo poema)\n",
        "        if re.match(r'^[IVXLCDM]+$', linea):\n",
        "            # Guardar el poema anterior si ya tenemos un título\n",
        "            if titulo_actual and poema_actual:\n",
        "                poemas_dict[titulo_actual] = '\\n'.join(poema_actual).strip()\n",
        "                poema_actual = []  # Reiniciar para el próximo poema\n",
        "\n",
        "            # Paso 4: Identificar el nuevo título en la línea siguiente\n",
        "            titulo_actual = poemas_lineas[i + 1].strip()\n",
        "            i += 1  # Saltar la línea del título ya procesada\n",
        "\n",
        "        # Paso 5: Agregar líneas al contenido del poema si estamos en un poema actual\n",
        "        elif titulo_actual:\n",
        "            poema_actual.append(linea)\n",
        "\n",
        "    # Paso 6: Guardar el último poema en el diccionario si existe\n",
        "    if titulo_actual and poema_actual:\n",
        "        poemas_dict[titulo_actual] = '\\n'.join(poema_actual).strip()\n",
        "\n",
        "    # Imprimir el primer poema para verificar el resultado\n",
        "    print(\"Procesamiento completo.\")\n",
        "    if poemas_dict:\n",
        "        primer_titulo = list(poemas_dict.keys())[0]\n",
        "        print(f\"Título del primer poema: {primer_titulo}\")\n",
        "        print(f\"Contenido del poema:\\n{poemas_dict[primer_titulo][:500]}\")\n",
        "    else:\n",
        "        print(\"No se encontraron poemas en el rango especificado.\")\n",
        "\n",
        "    # Listar los títulos de todos los poemas encontrados\n",
        "    print(\"\\nListado de títulos de los poemas encontrados:\\n\")\n",
        "    for i, title in enumerate(poemas_dict.keys(), start=1):\n",
        "        print(f\"{i}. {title}\")\n",
        "\n",
        "    return poemas_dict\n",
        "\n",
        "# Parámetros de entrada\n",
        "ruta_archivo = 'machado.txt'  # Ruta del archivo a procesar\n",
        "linea_inicio = 490  # Línea inicial\n",
        "linea_fin = 7363    # Línea final\n",
        "\n",
        "# Llamar a la función con los parámetros especificados\n",
        "poems_dict = procesar_poemas(ruta_archivo, linea_inicio, linea_fin)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjCZ_HZLjWvl",
        "outputId": "08eecc23-50cf-4448-fd3f-dd4b15a0bdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-07 13:03:51--  https://www.gutenberg.org/cache/epub/68525/pg68525.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 261733 (256K) [text/plain]\n",
            "Saving to: ‘machado.txt’\n",
            "\n",
            "machado.txt         100%[===================>] 255.60K  1.01MB/s    in 0.2s    \n",
            "\n",
            "2024-11-07 13:03:52 (1.01 MB/s) - ‘machado.txt’ saved [261733/261733]\n",
            "\n",
            "Leyendo el archivo...\n",
            "Extrayendo líneas desde 490 hasta 7363...\n",
            "Procesando líneas para extraer títulos y poemas...\n",
            "Procesamiento completo.\n",
            "Título del primer poema: EL VIAJERO\n",
            "Contenido del poema:\n",
            "EL VIAJERO\n",
            "\n",
            "Está en la sala familiar, sombría,\n",
            "y entre nosotros, el querido hermano\n",
            "que en el sueño infantil de un claro día\n",
            "vimos partir hacia un país lejano.\n",
            "\n",
            "Hoy tiene ya las sienes plateadas,\n",
            "un gris mechón sobre la angosta frente;\n",
            "y la fría inquietud de sus miradas\n",
            "revela un alma casi toda ausente.\n",
            "\n",
            "Deshójanse las copas otoñales\n",
            "del parque mustio y viejo.\n",
            "La tarde, tras los húmedos cristales,\n",
            "se pinta, y en el fondo del espejo,\n",
            "\n",
            "El rostro del hermano se ilumina\n",
            "suavemente. ¿Floridos desenga\n",
            "\n",
            "Listado de títulos de los poemas encontrados:\n",
            "\n",
            "1. EL VIAJERO\n",
            "2. EN EL ENTIERRO DE UN AMIGO\n",
            "3. RECUERDO INFANTIL\n",
            "4. ORILLAS DEL DUERO\n",
            "5. CANTE HONDO\n",
            "6. HORIZONTE\n",
            "7. EL POETA\n",
            "8. PRELUDIO\n",
            "9. DE LA VIDA\n",
            "10. INVENTARIO GALANTE\n",
            "11. LA NORIA\n",
            "12. EL CADALSO\n",
            "13. LAS MOSCAS\n",
            "14. ELEGÍA DE UN MADRIGAL\n",
            "15. ACASO...\n",
            "16. JARDÍN\n",
            "17. FANTASÍA DE UNA NOCHE DE ABRIL\n",
            "18. A UN NARANJO Y A UN LIMONERO\n",
            "19. LOS SUEÑOS MALOS\n",
            "20. HASTÍO\n",
            "21. CONSEJOS\n",
            "22. GLOSA\n",
            "23. SUEÑO INFANTIL\n",
            "24. CAMPO\n",
            "25. A UN VIEJO Y DISTINGUIDO SEÑOR\n",
            "26. LOS SUEÑOS\n",
            "27. RENACIMIENTO\n",
            "28. CABALLITOS\n",
            "29. RUIDOS\n",
            "30. PESADILLA\n",
            "31. SOL DE INVIERNO\n",
            "32. RETRATO\n",
            "33. A ORILLAS DEL DUERO\n",
            "34. POR TIERRAS DE ESPAÑA\n",
            "35. EL HOSPICIO\n",
            "36. EL DIOS IBERO\n",
            "37. LAS ENCINAS\n",
            "38. CAMINOS\n",
            "39. EN ABRIL, LAS AGUAS MIL\n",
            "40. UN LOCO\n",
            "41. FANTASÍA ICONOGRÁFICA\n",
            "42. UN CRIMINAL\n",
            "43. AMANECER DE OTOÑO\n",
            "44. EN TREN\n",
            "45. NOCHE DE VERANO\n",
            "46. PASCUA DE RESURRECCIÓN\n",
            "47. CAMPOS DE SORIA\n",
            "48. LA TIERRA DE ALVARGONZÁLEZ\n",
            "49. A UN OLMO SECO\n",
            "50. RECUERDOS\n",
            "51. AL MAESTRO «AZORÍN», POR SU LIBRO «CASTILLA»\n",
            "52. A JOSÉ MARÍA PALACIO\n",
            "53. OTRO VIAJE\n",
            "54. POEMA DE UN DÍA\n",
            "55. NOVIEMBRE, 1914\n",
            "56. LA SAETA\n",
            "57. DEL PASADO EFÍMERO\n",
            "58. LOS OLIVOS\n",
            "59. LLANTO DE LAS VIRTUDES\n",
            "60. LA MUJER MANCHEGA\n",
            "61. EL MAÑANA EFÍMERO\n",
            "62. PROVERBIOS Y CANTARES\n",
            "63. PARÁBOLAS\n",
            "64. MI BUFÓN\n",
            "65. A DON FRANCISCO GINER DE LOS RÍOS\n",
            "66. AL JOVEN MEDITADOR JOSÉ ORTEGA GASSET\n",
            "67. A XAVIER VALCARCE\n",
            "68. MARIPOSA DE LA SIERRA\n",
            "69. DESDE MI RINCÓN\n",
            "70. A UNA ESPAÑA JOVEN\n",
            "71. ESPAÑA, EN PAZ\n",
            "72. AL MAESTRO RUBÉN DARÍO\n",
            "73. A LA MUERTE DE RUBÉN DARÍO\n",
            "74. A NARCISO ALONSO CORTÉS, POETA DE CASTILLA\n",
            "75. MIS POETAS\n",
            "76. A DON MIGUEL DE UNAMUNO\n",
            "77. A JUAN RAMÓN JIMÉNEZ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Esta celda convierte el texto en secuencias numéricas de longitud fija para que el modelo aprenda patrones en el texto.\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Paso 1: Concatenar todos los poemas en un solo bloque de texto\n",
        "# Aquí unimos todos los poemas en una cadena larga para analizar el texto completo.\n",
        "all_poems_text = ' '.join(poems_dict.values())\n",
        "\n",
        "# Paso 2: Crear un conjunto único de caracteres\n",
        "# Esto obtiene todos los caracteres únicos presentes en el texto. El tamaño del conjunto será el vocabulario.\n",
        "chars = sorted(list(set(all_poems_text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Paso 3: Crear diccionarios de mapeo de caracteres a índices y viceversa\n",
        "# Estos diccionarios permiten convertir caracteres en índices y viceversa, facilitando el procesamiento en el modelo.\n",
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "\n",
        "# Paso 4: Convertir el texto a una secuencia de índices\n",
        "# Transformamos cada carácter del texto en su índice correspondiente según el diccionario `char_to_idx`.\n",
        "text_as_int = np.array([char_to_idx[char] for char in all_poems_text])\n",
        "\n",
        "# Paso 5: Definir la longitud de las secuencias y el paso entre ellas\n",
        "SEQ_LENGTH = 100  # Longitud de cada secuencia de entrada al modelo\n",
        "step = 1          # Número de caracteres que avanzamos para generar la siguiente secuencia\n",
        "\n",
        "# Paso 6: Crear listas para las secuencias y los caracteres siguientes\n",
        "# En cada paso, generamos una secuencia de longitud `SEQ_LENGTH` y almacenamos el carácter que sigue a esa secuencia.\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "# Bucle para generar secuencias y sus caracteres siguientes\n",
        "for i in range(0, len(text_as_int) - SEQ_LENGTH, step):\n",
        "    sequences.append(text_as_int[i:i + SEQ_LENGTH])       # Extraemos una secuencia de `SEQ_LENGTH` caracteres\n",
        "    next_chars.append(text_as_int[i + SEQ_LENGTH])        # Guardamos el carácter que sigue a la secuencia\n",
        "\n",
        "# Paso 7: Convertir las listas de secuencias y caracteres a tensores de PyTorch\n",
        "# `X` contiene las secuencias de entrada y `y` contiene el carácter objetivo a predecir para cada secuencia.\n",
        "X = torch.tensor(sequences, dtype=torch.long)\n",
        "y = torch.tensor(next_chars, dtype=torch.long)\n",
        "\n",
        "# Paso 8: Imprimir información sobre el procesamiento\n",
        "print(f\"Tamaño del vocabulario: {vocab_size}\")  # Tamaño del conjunto de caracteres únicos\n",
        "print(f\"Número de secuencias generadas: {X.size(0)}\")  # Número total de secuencias creadas para el entrenamiento\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFmBd2v_l3yd",
        "outputId": "69f29617-023a-4754-e355-bf7a9cab6634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del vocabulario: 84\n",
            "Número de secuencias generadas: 80022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b409516811ac>:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  X = torch.tensor(sequences, dtype=torch.long)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Modelo 1\n",
        "#Este código define una red neuronal recurrente (RNN). Este modelo usa embeddings y una capa LSTM\n",
        "\n",
        "# Este código define una red neuronal recurrente (RNN) con embeddings y una capa LSTM, con opciones para añadir capas adicionales.\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomizableRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=64, hidden_size=256, num_layers=1, dropout=0, activation='relu'):\n",
        "        super(CustomizableRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Capa de embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # LSTM personalizada\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "        # Capa fully connected (salida)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Función de activación\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        else:\n",
        "            self.activation = nn.Identity()  # Sin activación si no se especifica\n",
        "\n",
        "        # Opcional capas adicionales:\n",
        "        # self.fc1_adicional = nn.Linear(hidden_size, hidden_size)   # Capa fully connected adicional\n",
        "        # self.activation_fc1 = nn.ReLU()                            # Activación ReLU para capa adicional\n",
        "        # self.fc2_adicional = nn.Linear(hidden_size, hidden_size)   # Segunda capa fully connected adicional\n",
        "        # self.activation_fc2 = nn.ReLU()                            # Activación ReLU para segunda capa\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # Paso a través de la capa de embeddings\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Paso por LSTM\n",
        "        out, h = self.lstm(x, h)\n",
        "\n",
        "        # Aplicar función de activación (si se especificó)\n",
        "        out = self.activation(out[:, -1, :])\n",
        "\n",
        "        # Paso por capas adicionales si se descomentan\n",
        "        # out = self.fc1_adicional(out)             # Paso por primera capa fully connected adicional\n",
        "        # out = self.activation_fc1(out)            # Aplicar activación ReLU\n",
        "        # out = self.fc2_adicional(out)             # Paso por segunda capa fully connected adicional\n",
        "        # out = self.activation_fc2(out)            # Aplicar activación ReLU\n",
        "\n",
        "        # Paso final por la capa fully connected de salida\n",
        "        out = self.fc(out)\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Inicializar el estado oculto (hidden state y cell state) con ceros\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
        "\n",
        "# Ejemplo de hiperparámetros\n",
        "embed_size = 128       # Tamaño del embedding\n",
        "hidden_size = 512      # Tamaño de la capa oculta\n",
        "num_layers = 2         # Número de capas LSTM\n",
        "dropout = 0.3          # Dropout para evitar sobreajuste\n",
        "activation = 'tanh'    # Función de activación\n",
        "\n",
        "# Crear el modelo con los hiperparámetros personalizables\n",
        "model = CustomizableRNN(vocab_size, embed_size, hidden_size, num_layers, dropout, activation)\n",
        "\n",
        "# Imprimir el modelo para verificar la estructura\n",
        "print(model)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqmE7Be2aJEd",
        "outputId": "4e08e892-97ae-4292-fd0f-31a06b0a98d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomizableRNN(\n",
            "  (embedding): Embedding(84, 128)\n",
            "  (lstm): LSTM(128, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (fc): Linear(in_features=512, out_features=84, bias=True)\n",
            "  (activation): Tanh()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de entrenamiento con cálculo de precisión\n",
        "def train(model, X, y, epochs=5, batch_size=64):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # Bucle de entrenamiento\n",
        "        for i in range(0, len(X) - batch_size, batch_size):\n",
        "            inputs = X[i:i+batch_size]\n",
        "            targets = y[i:i+batch_size]\n",
        "\n",
        "            # Inicializar el estado oculto en el dispositivo correcto\n",
        "            h = model.init_hidden(batch_size)\n",
        "            h = tuple([each.to(device) for each in h])  # Asegurar que h esté en el mismo dispositivo que el modelo\n",
        "\n",
        "            # Reseteo del optimizador y desconectar el gradiente del estado oculto\n",
        "            optimizer.zero_grad()\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # Forward pass\n",
        "            output, h = model(inputs, h)\n",
        "            loss = criterion(output, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Acumular pérdida total\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calcular precisión\n",
        "            preds = output.argmax(dim=1)  # Predicción con mayor probabilidad\n",
        "            correct_predictions += (preds == targets).sum().item()  # Comparar predicciones correctas\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "        avg_loss = total_loss / (len(X) // batch_size)\n",
        "        accuracy = correct_predictions / total_samples\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Pérdida promedio: {avg_loss:.4f}, Precisión: {accuracy:.4f}\")\n",
        "\n",
        "print(\"Función de entrenamiento creada \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BNOFdjXmHEU",
        "outputId": "691615f9-f8cb-4a61-fec9-dad3a86846aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función de entrenamiento creada \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### modelo 1\n",
        "\n",
        "# recordar si lo hacemos muy grande, se nos cuelga la maquina\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Inicialización de la función de pérdida y optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Configurar el dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "X, y = X.to(device), y.to(device)\n",
        "\n",
        "# Entrenar el modelo\n",
        "train(model, X, y, epochs=epochs, batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adJLlS1CaJ00",
        "outputId": "59b20604-8eca-4fb8-b9d6-a89e04d0d86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Pérdida promedio: 2.2370, Precisión: 0.3286\n",
            "Epoch [2/10], Pérdida promedio: 1.9205, Precisión: 0.4068\n",
            "Epoch [3/10], Pérdida promedio: 1.7772, Precisión: 0.4487\n",
            "Epoch [4/10], Pérdida promedio: 1.6668, Precisión: 0.4809\n",
            "Epoch [5/10], Pérdida promedio: 1.5765, Precisión: 0.5078\n",
            "Epoch [6/10], Pérdida promedio: 1.4996, Precisión: 0.5295\n",
            "Epoch [7/10], Pérdida promedio: 1.4348, Precisión: 0.5485\n",
            "Epoch [8/10], Pérdida promedio: 1.3753, Precisión: 0.5648\n",
            "Epoch [9/10], Pérdida promedio: 1.3260, Precisión: 0.5787\n",
            "Epoch [10/10], Pérdida promedio: 1.2772, Precisión: 0.5923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_text, length=300):\n",
        "    model.eval()  # Cambiar a modo evaluación\n",
        "    generated = seed_text\n",
        "    h = model.init_hidden(1)  # Estado oculto inicial con batch_size = 1\n",
        "    h = tuple([each.to(device) for each in h])  # Asegurarse de que el estado oculto esté en el dispositivo correcto\n",
        "\n",
        "    # Convertir la semilla a índices\n",
        "    input_seq = torch.tensor([char_to_idx[char] for char in seed_text], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    for _ in range(length):\n",
        "        output, h = model(input_seq, h)\n",
        "        prob = output.softmax(dim=1).data\n",
        "        char_idx = torch.multinomial(prob, 1).item()\n",
        "        char = idx_to_char[char_idx]\n",
        "        generated += char\n",
        "\n",
        "        # Usar el nuevo carácter generado como entrada para el próximo paso\n",
        "        input_seq = torch.tensor([[char_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated\n",
        "print(\"Funcion generar texto creada\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFJpzKEzndB6",
        "outputId": "611c8c8b-074e-459b-8353-a3123aea665c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funcion generar texto creada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar un nuevo poema a partir de una semilla con el modelo 1\n",
        "seed_text = \"El viento susurra \"\n",
        "generated_poem = generate_text(model, seed_text.lower(), length=300)\n",
        "print(\"Poema generado modelo :\\n\")\n",
        "print(generated_poem)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnPjip7IBOUm",
        "outputId": "0bc9efcb-0e2d-4030-c499-feeedf23649c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poema generado modelo :\n",
            "\n",
            "el viento susurra hacha\n",
            "de España del que de un lameo.\n",
            "\n",
            "Tiene el jardín al ángel viento\n",
            "del agua de Ciruz, lleva la sombra paz sin púmo del ver manchito.\n",
            "\n",
            "Huye del viejo ahuyente al fon y la guerra,\n",
            "aldamas y aquí, hoy ayer, y Azorín,\n",
            "paba era quiere Marís.\n",
            "\n",
            "(Bonzario el águila a mismo.\n",
            "La bunda quiere, parda el mar \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 1: Modificar el número de épocas en ambos modelos.\n",
        "# ¿Observas que aumentar las épocas siempre mejora el texto generado, o llega un punto en el que la calidad deja de mejorar?\n",
        "\n",
        "# TODO 2: Aplcia \"early stopping\" .\n",
        "# ¿Cuántas épocas sin mejoras necesitas para detener el entrenamiento? ¿Crees que esto mejora la eficiencia sin comprometer la calidad?\n",
        "\n",
        "# TODO 3: Ajusta la longitud de las secuencias (SEQ_LENGTH).\n",
        "# ¿Una longitud de secuencia más larga o más corta produce un texto más coherente?\n",
        "\n",
        "# TODO 4: Cambia la función de activación en el CustomizableRNN (Modelo 2) entre relu, tanh, y Identity\n",
        "# ¿Qué función de activación parece capturar mejor el estilo de los poemas? ¿Por qué crees que ocurre esto?\n",
        "\n",
        "# TODO 5: Disminuye el tamaño de hidden_size en ambos modelos\n",
        "# ¿El modelo sigue generando texto coherente con una capa oculta más pequeña? ¿En qué cambia la calidad del texto?\n",
        "\n",
        "# TODO 6: Añade una capa de dropout en el SimpleRNN y observa el efecto en la generalización del modelo.\n",
        "# ¿Cómo afecta el dropout al texto generado? ¿Notas menos repetición en el texto, o algún cambio en la coherencia?\n",
        "\n",
        "# TODO 7: Cambia la frase inicial de generación de texto (seed_text y analiza cómo afecta el estilo y coherencia del texto generado.\n",
        "# ¿La semilla inicial tiene un impacto importante en el estilo del texto? ¿Qué pasa si pruebas con una frase fuera del tema de los poemas?\n",
        "\n",
        "# TODO 8: Cambia el batch_size para observar el impacto en la velocidad de entrenamiento y en la pérdida final.\n",
        "# ¿Entrenar con un tamaño de batch más pequeño afecta la convergencia de la pérdida y la calidad del texto?\n",
        "\n",
        "# TODO 9:  Ajusta la longitud de texto generado (length)\n",
        "# ¿El modelo logra mantener coherencia en frases más largas? ¿Cómo cambia el estilo y la coherencia según la longitud?\n",
        "\n",
        "# TODO 10: Añade gráficos de pérdida por época para ambos modelos\n",
        "# ¿El gráfico de pérdida muestra una tendencia estable? ¿Existen caídas o aumentos repentinos y a qué podrían deberse?\n"
      ],
      "metadata": {
        "id": "HK4r0N6ySaZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complementario"
      ],
      "metadata": {
        "id": "LHitp5ztgUBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modificar el modelo para que prediga por palabras en lugar de caracteres.\n",
        "# Cambia la lógica de preprocesamiento para trabajar con palabras en vez de caracteres.\n",
        "# Ajusta el vocabulario para mapear palabras a índices (word_to_idx) y viceversa.\n",
        "# Considera cómo cambiará SEQ_LENGTH, ya que ahora representará el número de palabras.\n",
        "# Analiza si el texto generado es más coherente al trabajar a nivel de palabras."
      ],
      "metadata": {
        "id": "sRsTnkJZgRpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerías necesarias\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import torch.nn as nn\n",
        "\n",
        "# Preprocesamos el texto\n",
        "all_poems_text = ' '.join(poems_dict.values())\n",
        "all_poems_text = all_poems_text.lower()\n",
        "all_poems_text = re.sub(r'[^a-záéíóúüñ\\s]', '', all_poems_text)\n",
        "\n",
        "# Separamos el texto en palabras\n",
        "words = all_poems_text.split()\n",
        "\n",
        "# Creamos el vocabulario y los diccionarios\n",
        "vocab = sorted(set(words))\n",
        "vocab_size = len(vocab)\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Convertimos el texto en índices\n",
        "text_as_int = [word_to_idx[word] for word in words]\n",
        "\n",
        "# Definimos la longitud de las secuencias\n",
        "SEQ_LENGTH = 10\n",
        "\n",
        "# Creamos las secuencias y los targets\n",
        "sequences = []\n",
        "next_words = []\n",
        "for i in range(0, len(text_as_int) - SEQ_LENGTH):\n",
        "    sequences.append(text_as_int[i:i + SEQ_LENGTH])\n",
        "    next_words.append(text_as_int[i + SEQ_LENGTH])\n",
        "\n",
        "# Convertimos a tensores\n",
        "X = torch.tensor(sequences, dtype=torch.long)\n",
        "y = torch.tensor(next_words, dtype=torch.long)\n",
        "\n",
        "print(f\"Tamaño del vocabulario: {vocab_size}\")\n",
        "print(f\"Número de secuencias: {X.size(0)}\")\n",
        "\n",
        "# Definimos el modelo RNN\n",
        "class CustomizableRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=128, hidden_size=512, num_layers=2, dropout=0.3, activation='tanh'):\n",
        "        super(CustomizableRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Capa de embeddings: convierte palabras en vectores\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        # LSTM para procesar secuencias\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        # Capa fully connected para la salida\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Función de activación\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        else:\n",
        "            self.activation = nn.Identity()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # x es la entrada, h es el estado oculto\n",
        "        x = self.embedding(x)\n",
        "        out, h = self.lstm(x, h)\n",
        "        out = self.activation(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Inicializamos el estado oculto y de memoria\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
        "\n",
        "# Hiperparámetros\n",
        "embed_size = 128\n",
        "hidden_size = 512\n",
        "num_layers = 2\n",
        "dropout = 0.3\n",
        "activation = 'tanh'\n",
        "\n",
        "# Creamos el modelo\n",
        "model = CustomizableRNN(vocab_size, embed_size, hidden_size, num_layers, dropout, activation)\n",
        "\n",
        "# Definimos pérdida y optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Usamos GPU si está disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "X, y = X.to(device), y.to(device)\n",
        "\n",
        "# Función de entrenamiento con precisión\n",
        "def train(model, X, y, epochs=5, batch_size=64):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "        for i in range(0, len(X) - batch_size, batch_size):\n",
        "            inputs = X[i:i+batch_size]\n",
        "            targets = y[i:i+batch_size]\n",
        "\n",
        "            # Inicializamos el estado oculto\n",
        "            h = model.init_hidden(batch_size)\n",
        "            h = tuple([each.to(device) for each in h])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # Forward pass\n",
        "            output, h = model(inputs, h)\n",
        "            loss = criterion(output, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Cálculo de precisión\n",
        "            preds = output.argmax(dim=1)\n",
        "            correct_predictions += (preds == targets).sum().item()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "        avg_loss = total_loss / (len(X) // batch_size)\n",
        "        accuracy = correct_predictions / total_samples\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Pérdida promedio: {avg_loss:.4f}, Precisión: {accuracy:.4f}\")\n",
        "\n",
        "# Entrenamos el modelo\n",
        "batch_size = 32\n",
        "epochs = 30\n",
        "train(model, X, y, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Función para generar texto\n",
        "def generate_text(model, seed_text, length=50):\n",
        "    model.eval()\n",
        "    generated = seed_text\n",
        "    h = model.init_hidden(1)\n",
        "    h = tuple([each.to(device) for each in h])\n",
        "\n",
        "    seed_words = seed_text.lower().split()\n",
        "    seed_indices = [word_to_idx.get(word, 0) for word in seed_words]\n",
        "    input_seq = torch.tensor([seed_indices], dtype=torch.long).to(device)\n",
        "\n",
        "    for _ in range(length):\n",
        "        # Generamos la siguiente palabra\n",
        "        output, h = model(input_seq, h)\n",
        "        prob = output.softmax(dim=1).data\n",
        "        word_idx = torch.multinomial(prob, 1).item()\n",
        "        word = idx_to_word[word_idx]\n",
        "        generated += ' ' + word\n",
        "\n",
        "        # Actualizamos la secuencia de entrada\n",
        "        input_seq = torch.tensor([[word_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsfyjXJKfyDw",
        "outputId": "9b9c30eb-d63c-4326-c608-efa787a08783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del vocabulario: 3852\n",
            "Número de secuencias: 13992\n",
            "Epoch [1/30], Pérdida promedio: 6.9718, Precisión: 0.0534\n",
            "Epoch [2/30], Pérdida promedio: 6.1972, Precisión: 0.0546\n",
            "Epoch [3/30], Pérdida promedio: 5.9394, Precisión: 0.0545\n",
            "Epoch [4/30], Pérdida promedio: 5.6999, Precisión: 0.0611\n",
            "Epoch [5/30], Pérdida promedio: 5.4233, Precisión: 0.0726\n",
            "Epoch [6/30], Pérdida promedio: 5.1684, Precisión: 0.0799\n",
            "Epoch [7/30], Pérdida promedio: 4.9028, Precisión: 0.0861\n",
            "Epoch [8/30], Pérdida promedio: 4.6647, Precisión: 0.0910\n",
            "Epoch [9/30], Pérdida promedio: 4.4826, Precisión: 0.0993\n",
            "Epoch [10/30], Pérdida promedio: 4.2957, Precisión: 0.1104\n",
            "Epoch [11/30], Pérdida promedio: 4.1561, Precisión: 0.1241\n",
            "Epoch [12/30], Pérdida promedio: 3.8625, Precisión: 0.1512\n",
            "Epoch [13/30], Pérdida promedio: 3.6117, Precisión: 0.1904\n",
            "Epoch [14/30], Pérdida promedio: 3.3779, Precisión: 0.2283\n",
            "Epoch [15/30], Pérdida promedio: 3.1350, Precisión: 0.2720\n",
            "Epoch [16/30], Pérdida promedio: 3.0265, Precisión: 0.2908\n",
            "Epoch [17/30], Pérdida promedio: 2.8777, Precisión: 0.3096\n",
            "Epoch [18/30], Pérdida promedio: 2.6992, Precisión: 0.3495\n",
            "Epoch [19/30], Pérdida promedio: 2.4640, Precisión: 0.4037\n",
            "Epoch [20/30], Pérdida promedio: 2.2020, Precisión: 0.4652\n",
            "Epoch [21/30], Pérdida promedio: 1.9990, Precisión: 0.5115\n",
            "Epoch [22/30], Pérdida promedio: 1.8271, Precisión: 0.5512\n",
            "Epoch [23/30], Pérdida promedio: 1.6524, Precisión: 0.5931\n",
            "Epoch [24/30], Pérdida promedio: 1.4917, Precisión: 0.6345\n",
            "Epoch [25/30], Pérdida promedio: 1.3314, Precisión: 0.6745\n",
            "Epoch [26/30], Pérdida promedio: 1.2058, Precisión: 0.7099\n",
            "Epoch [27/30], Pérdida promedio: 1.0815, Precisión: 0.7427\n",
            "Epoch [28/30], Pérdida promedio: 0.9730, Precisión: 0.7679\n",
            "Epoch [29/30], Pérdida promedio: 0.8812, Precisión: 0.7902\n",
            "Epoch [30/30], Pérdida promedio: 0.8368, Precisión: 0.8026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos texto con el modelo entrenado\n",
        "seed_text = \"El viento susurra\"\n",
        "generated_poem = generate_text(model, seed_text, length=50)\n",
        "print(\"\\nPoema generado:\\n\")\n",
        "print(generated_poem)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0Vcap17nlES",
        "outputId": "2b60c4cf-8633-4fed-f90b-c9b76d604b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Poema generado:\n",
            "\n",
            "El viento susurra casa ardían violetas las infecto largo jiménez sus lo claro le también se señor de los campos se también a la palabra buena del hombre del río ni eres tú le noche de quijote el aquel profesor cual la haces galería y caminar donde él es el vida verde por\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ¿La generación por palabras afecta la fluidez del texto? ¿Qué ventajas o desventajas encuentras?\n",
        "# ¿Hay sobreajuste? ¿Como lo solucionaria?"
      ],
      "metadata": {
        "id": "BrB25h9WgNoE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}